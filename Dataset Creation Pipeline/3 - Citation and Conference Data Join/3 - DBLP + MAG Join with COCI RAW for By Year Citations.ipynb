{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation and Conference Data Join - DBLP + MAG and COCI\n",
    "\n",
    "Jupyter Notebook for the join of the conferences and location data between the DBLP + MAG and COCI dumps.\n",
    "\n",
    "For this process, the following CSV files are needed: ```out_coci_citations_count.csv``` and ```out_dblp_and_mag_joined.csv```. <br>\n",
    "The first must be generated running the Notebook ```preprocess_opencitations.ipynb``` that is contained in the ```1 - Citation Dumps Preprocess``` folder of this project.\n",
    "The above files must be generated running the ```1 - DBLP and MAG Data Join Notebook.ipynb``` Notebook that is contained in the same folder as this Notebook.\n",
    "\n",
    "In particular, the following operations are going to be executed:\n",
    "* Opening of the CSV preprocessed dumps\n",
    "* Join between the two datasets\n",
    "* Drop of the useless columns\n",
    "* Fix of the mismatched data types\n",
    "\n",
    "Lastly, the entire preprocessed dump is going to be saved on disk in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "Please set your working directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PATHS ********************+\n",
    "\n",
    "# Dumps Directory Path\n",
    "path_file_import = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Import/COCI_RAW/'\n",
    "\n",
    "# CSV Exports Directory Path\n",
    "path_file_export = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine New Data with a \"Partial\" CSV\n",
    "\n",
    "This can be really useful in case of limited disk space, allowing us to partially process the dump (using a subset of the CSVs) and free some space on disk by deleting the CSVs that have been already processed.\n",
    "\n",
    "**Note**: the delete operations need to be made manually\n",
    "**Note**: the partial CSV needs to be in the same format of the one generated with this script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_with_partial_csv = False\n",
    "partial_csv_path = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read of the DBLP + MAG CSV Joined Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_with_partial_csv:\n",
    "    df_joined = pd.read_csv(partial_csv_path + 'out_citations_by_year_and_conferences.csv', low_memory=False)\n",
    "    print(f'Successfully Imported the Partial CSV')\n",
    "else:\n",
    "    df_joined = pd.read_csv(path_file_export + 'out_dblp_and_mag_joined.csv', low_memory=False, index_col=[0])\n",
    "    print(f'Successfully Imported the DBLP + MAG CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Support Dataframe\n",
    "It's going to help us extracting the citation' year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combine_with_partial_csv:\n",
    "    # Drop of the useless mag citations column\n",
    "    df_joined = df_joined.drop(columns=['CitationCount_Mag', 'CitationCount_MagEstimated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the columns that are going to contain the citation obtained by a paper during a specific year. Also, needed for filtering the COCI paper that are not contained neither and MAG or DBLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_support_empty = df_joined.copy()\n",
    "\n",
    "# Drop of the useless column\n",
    "df_support_empty = df_support_empty.drop(columns=['ConferenceLocation', 'ConferenceNormalizedName', 'ConferenceTitle', 'OriginalTitle'])\n",
    "\n",
    "# Creation of the support column\n",
    "df_support_empty['Year_of_Citation'] = np.nan\n",
    "df_support_empty.rename(columns={'Year': 'Year_of_Publication'}, inplace=True)\n",
    "df_support_empty = df_support_empty.reindex(sorted(df_support_empty.columns), axis=1)\n",
    "\n",
    "df_support_empty.loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Year Citation Columns to the Original Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combine_with_partial_csv:\n",
    "    \n",
    "    start_year = 1950 # Probably there aren't citations before this date. We'll drop the empty columns later\n",
    "    actual_year = date.today().year\n",
    "\n",
    "    for i in range(start_year, actual_year + 1):\n",
    "        df_joined[str(i)] = 0\n",
    "\n",
    "df_joined.loc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Join of the COCI Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All Files' Names\n",
    "coci_all_csvs = glob.glob(path_file_import + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for current_csv_name in coci_all_csvs:\n",
    "\n",
    "    # Empty the support dataframe\n",
    "    df_support = df_support_empty.copy()\n",
    "\n",
    "    # Open the current CSV\n",
    "    print(f'Currently processing CSV {count}: {current_csv_name}')\n",
    "    count += 1\n",
    "    df_coci_current_csv = pd.read_csv(current_csv_name, low_memory=False)\n",
    "\n",
    "    # Drop of the useless columns: 'oci', 'citing', 'creation', 'journal_sc', 'author_sc'\n",
    "    df_coci_current_csv = df_coci_current_csv.drop(columns=['oci', 'citing', 'creation', 'journal_sc', 'author_sc'])\n",
    "\n",
    "    # Column rename\n",
    "    df_coci_current_csv = df_coci_current_csv.rename(columns={'cited': 'Doi'})\n",
    "\n",
    "    # Making sure that everything has the same format\n",
    "    df_coci_current_csv.Doi = df_coci_current_csv.Doi.str.lower()\n",
    "\n",
    "    # Join with the support dataframe\n",
    "    df_support = pd.merge(df_support, df_coci_current_csv, on=['Doi'], how='inner')\n",
    "\n",
    "    # Filtering the rows with a negative timespan\n",
    "    df_support.timespan = df_support[\"timespan\"].astype(str)\n",
    "    df_support = df_support[~df_support[\"timespan\"].str.contains('-')]\n",
    "\n",
    "    # Computing the citation's year\n",
    "    df_support.Year_of_Citation = df_support.timespan.str.split('Y').str[0].str.split('P').str[1]\n",
    "    df_support = df_support.dropna(subset=['Year_of_Citation']) # Drop of the broken records\n",
    "    df_support.Year_of_Citation = df_support.Year_of_Citation.astype(int) + df_support.Year_of_Publication.astype(int)\n",
    "\n",
    "    # Removing the broken records\n",
    "    df_support = df_support.loc[(df_support['Year_of_Citation'] <= date.today().year)] \n",
    "\n",
    "    # Reshaping the dataframe and resetting its index\n",
    "    df_support_reshaped = pd.crosstab(df_support.Doi, df_support.Year_of_Citation)\n",
    "    df_support_reshaped = df_support_reshaped.reset_index()\n",
    "\n",
    "    # Fixing the column name type\n",
    "    for column in df_support_reshaped:\n",
    "        df_support_reshaped.rename(columns = {column: str(column)}, inplace=True)\n",
    "\n",
    "    # Join with the original dataframe\n",
    "    df_joined = pd.merge(df_joined, df_support_reshaped, on=['Doi'], how='left')\n",
    "\n",
    "    # Sum of the citation counts values\n",
    "    for column in df_joined:\n",
    "        if '_x' in str(column):\n",
    "            coci_column = str(column).split('_x')[0] + '_y'\n",
    "\n",
    "            # Replacing nan with zeros in the coci rows that didn't match\n",
    "            df_joined[coci_column] = df_joined[coci_column].fillna(0).astype(int)\n",
    "\n",
    "            # Column sum\n",
    "            df_joined[column] += df_joined[coci_column]\n",
    "            \n",
    "            # Column rename and drop\n",
    "            df_joined.rename(columns = {column: str(column).split('_x')[0]}, inplace=True)\n",
    "            df_joined = df_joined.drop(columns=[coci_column])\n",
    "\n",
    "# Export of the final dataframe\n",
    "#df_joined.to_csv(path_file_export + 'out_citations_by_year_and_conferences.csv')\n",
    "#print(f'Successfully Exported the Joined CSV to {path_file_export}out_citations_by_year_and_conferences.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_support_cp = df_support.copy()\n",
    "df_support_cp = df_support_cp[~df_support_cp[\"timespan\"].str.contains('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_cp = df_joined.copy()\n",
    "\n",
    "# Removing the broken records\n",
    "df_support = df_support.loc[(df_support['Year_of_Citation'] <= date.today().year)]\n",
    "df_support = df_support[~df_support[\"timespan\"].str.contains('-')]\n",
    "\n",
    "# Reshaping the dataframe and resetting its index\n",
    "df_support_reshaped = pd.crosstab(df_support.Doi, df_support.Year_of_Citation)\n",
    "df_support_reshaped = df_support_reshaped.reset_index()\n",
    "\n",
    "# Fixing the column name type\n",
    "for column in df_support_reshaped:\n",
    "    df_support_reshaped.rename(columns = {column: str(column)}, inplace=True)\n",
    "\n",
    "# Join with the original dataframe\n",
    "df_joined_cp = pd.merge(df_joined_cp, df_support_reshaped, on=['Doi'], how='inner')\n",
    "\n",
    "# Sum of the citation counts values\n",
    "for column in df_joined_cp:\n",
    "    if '_x' in str(column):\n",
    "        # Column sum\n",
    "        df_joined_cp[column] += df_joined_cp[str(column).split('_x')[0] + '_y']\n",
    "        \n",
    "        # Column rename and drop\n",
    "        df_joined_cp.rename(columns = {column: str(column).split('_x')[0]}, inplace=True)\n",
    "        df_joined_cp = df_joined_cp.drop(columns=[str(column).split('_x')[0] + '_y'])\n",
    "\n",
    "df_joined_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_cp = df_joined.copy()\n",
    "\n",
    "# Removing the broken records\n",
    "df_support = df_support.loc[(df_support['Year_of_Citation'] <= date.today().year)]\n",
    "df_support = df_support[~df_support[\"timespan\"].str.contains('-')]\n",
    "\n",
    "# Reshaping the dataframe and resetting its index\n",
    "df_support_reshaped = pd.crosstab(df_support.Doi, df_support.Year_of_Citation)\n",
    "df_support_reshaped = df_support_reshaped.reset_index()\n",
    "\n",
    "# Fixing the column name type\n",
    "for column in df_support_reshaped:\n",
    "    df_support_reshaped.rename(columns = {column: str(column)}, inplace=True)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    # Join with the original dataframe\n",
    "    df_joined_cp = pd.merge(df_joined_cp, df_support_reshaped, on=['Doi'], how='inner')\n",
    "\n",
    "    # Sum of the citation counts values\n",
    "    for column in df_joined_cp:\n",
    "        if '_x' in str(column):\n",
    "            # Column sum\n",
    "            df_joined_cp[column] += df_joined_cp[str(column).split('_x')[0] + '_y']\n",
    "            \n",
    "            # Column rename and drop\n",
    "            df_joined_cp.rename(columns = {column: str(column).split('_x')[0]}, inplace=True)\n",
    "            df_joined_cp = df_joined_cp.drop(columns=[str(column).split('_x')[0] + '_y'])\n",
    "\n",
    "df_joined_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_support_reshaped = df_support_reshaped.reset_index()\n",
    "df_support_reshaped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_support = df_support.loc[(df_support['Year_of_Citation'] <= date.today().year)]\n",
    "df_support = df_support[~df_support[\"timespan\"].str.contains('-')]\n",
    "df_support_reshaped = pd.crosstab(df_support.Doi, df_support.Year_of_Citation)\n",
    "\n",
    "df_support_reshaped = df_support_reshaped.reset_index()\n",
    "\n",
    "\n",
    "for column in df_support_reshaped:\n",
    "    df_support_reshaped.rename(columns = {column: str(column)}, inplace=True)\n",
    "\n",
    "for column in df_support_reshaped:\n",
    "    print(type(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_joined_cp:\n",
    "    print(type(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_cp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_support_reshaped[\"2020\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Avoid duplicate column name error\n",
    "        original_column_new_name = str(column).split('_x')[0] + '_original'\n",
    "        column_to_be_summed_new_name = str(column).split('_x')[0] + '_to_be_summed'\n",
    "        df_joined_cp.rename(columns = {column: original_column_new_name}, inplace=True)\n",
    "        df_joined_cp.rename(columns = {str(column).split('_x')[0] + '_y': column_to_be_summed_new_name}, inplace=True)\n",
    "    \n",
    "        df_joined_cp = df_joined_cp.reindex(sorted(df_joined_cp.columns), axis=1)\n",
    "\n",
    "        # Column sum\n",
    "        df_joined_cp[original_column_new_name] += df_joined_cp[column_to_be_summed_new_name]\n",
    "\n",
    "        # Column rename and drop\n",
    "        df_joined_cp.rename(columns = {original_column_new_name: str(original_column_new_name).split('_original')[0]}, inplace=True)\n",
    "        df_joined_cp = df_joined_cp.drop(columns=[column_to_be_summed_new_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coci_current_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by citations count descending to see the articles with the most citations\n",
    "df_coci_current_csv = df_coci_current_csv.sort_values(by='citations_count', ascending=False)\n",
    "df_coci_current_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_coci_index, df_coci_row in df_coci_current_csv.iterrows():\n",
    "    df_joined.loc[(df_joined.Doi == df_coci_row['Doi']), str(df_coci_row['Year_of_Citation'])] += df_coci_row['citations_count'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funziona ma Ã¨ lentissima: 40 ore per file...\n",
    "\n",
    "print_counter = 0\n",
    "    total_row_count = df_joined.index.__len__()\n",
    "    for df_joined_index, df_joined_row in df_joined.iterrows():\n",
    "\n",
    "        print_counter += 1\n",
    "        if print_counter == 1000:\n",
    "            print(f\"Row {df_joined_index + 1} of {total_row_count}\")\n",
    "            print_counter = 0\n",
    "\n",
    "        try:\n",
    "            coci_rows = df_coci_current_csv.loc[[(df_coci_current_csv.Doi == df_joined_row['Doi'])]]\n",
    "            print(coci_rows)\n",
    "\n",
    "            for df_coci_index, df_coci_row in coci_rows.iterrows():\n",
    "                df_joined.at[df_joined_index, str(df_coci_row['Year_of_Citation'])] = df_coci_row['citations_count']\n",
    "        \n",
    "            df_coci_current_csv.drop(df_coci_current_csv.loc[df_coci_current_csv['Doi'] == df_joined_row['Doi']].index, inplace=True)\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_row_count = df_coci_current_csv.index.__len__()\n",
    "for df_coci_index, df_coci_row in df_coci_current_csv.iterrows():\n",
    "\n",
    "    if df_coci_index % 1000 == 0:\n",
    "        print(f\"Row {df_coci_index} of {total_row_count}\")\n",
    "\n",
    "    df_joined.loc[(df_joined.Doi == df_coci_row['Doi']), str(df_coci_row['Year_of_Citation'])] = df_coci_row['citations_count'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.index.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_counter = 0\n",
    "    for df_joined_index, df_joined_row in df_joined.iterrows():\n",
    "\n",
    "        print_counter += 1\n",
    "        if print_counter == 25000:\n",
    "            print(f\"Riga numero {dblp_index + 1}\")\n",
    "            print_counter = 0\n",
    "\n",
    "        match = False\n",
    "\n",
    "        for df_coci_index, df_coci_row in df_coci_current_csv.iterrows():\n",
    "\n",
    "            if df_joined_row['Doi'] == df_coci_row['Doi']:\n",
    "                df_joined.at[df_joined_index, str(df_coci_row['Year_of_Citation'])] = df_coci_row['citations_count']\n",
    "\n",
    "                match = True\n",
    "                break\n",
    "        \n",
    "        # If we got a match, we remove the row to speed up the next search\n",
    "        if match:\n",
    "            df_coci_current_csv.drop([df_coci_index, df_coci_index], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the CSV Preprocessed COCI Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the article column to doi and making sure that everything is in lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coci = df_coci.rename(columns={'article': 'Doi'})\n",
    "df_coci = df_coci.reindex(sorted(df_coci.columns), axis=1)\n",
    "\n",
    "df_coci.Doi = df_coci.Doi.str.lower()\n",
    "df_coci.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Between DBLP+MAG and COCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that all dois are in lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coci.Doi = df_coci.Doi.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dblp_and_mag = pd.merge(df_dblp_and_mag, df_coci, on=['Doi'], how='left')\n",
    "\n",
    "df_dblp_and_mag.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column rename and sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dblp_and_mag.rename(columns={'citations_count': 'CitationCount_COCI'}, inplace=True)\n",
    "df_dblp_and_mag = df_dblp_and_mag.reindex(sorted(df_dblp_and_mag.columns), axis=1)\n",
    "df_dblp_and_mag.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the NaN Citations to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dblp_and_mag['CitationCount_COCI'] = df_dblp_and_mag['CitationCount_COCI'].fillna(0)\n",
    "df_dblp_and_mag['CitationCount_Mag'] = df_dblp_and_mag['CitationCount_Mag'].fillna(0)\n",
    "df_dblp_and_mag['CitationCount_MagEstimated'] = df_dblp_and_mag['CitationCount_MagEstimated'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix of the data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dblp_and_mag = df_dblp_and_mag.astype({\"CitationCount_COCI\": int}) \n",
    "df_dblp_and_mag = df_dblp_and_mag.astype({\"CitationCount_Mag\": int}) \n",
    "df_dblp_and_mag = df_dblp_and_mag.astype({\"CitationCount_MagEstimated\": int}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dblp_and_mag.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write of the Final CSV on Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the resulting dataframe on disk in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write of the resulting CSV on Disk\n",
    "df_dblp_and_mag.to_csv(path_file_export + 'out_citations_and_conferences.csv')\n",
    "print(f'Successfully Exported the Processed CSV to {path_file_export}out_citations_and_conferences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check of the Exported CSV to be sure that everything went fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of the Exported CSV\n",
    "df_joined_exported_csv = pd.read_csv(path_file_export + 'out_citations_and_conferences.csv', low_memory=False, index_col=[0])\n",
    "df_joined_exported_csv"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
