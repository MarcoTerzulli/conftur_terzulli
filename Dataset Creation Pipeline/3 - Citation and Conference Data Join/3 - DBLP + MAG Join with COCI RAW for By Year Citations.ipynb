{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation and Conference Data Join - DBLP + MAG and COCI\n",
    "\n",
    "Jupyter Notebook for the join of the conferences and location data between the DBLP + MAG and COCI dumps.\n",
    "\n",
    "For this process, the following CSV files are needed: ```out_coci_citations_count.csv``` and ```out_dblp_and_mag_joined.csv```. <br>\n",
    "The first must be generated running the Notebook ```preprocess_opencitations.ipynb``` that is contained in the ```1 - Citation Dumps Preprocess``` folder of this project.\n",
    "The above files must be generated running the ```1 - DBLP and MAG Data Join Notebook.ipynb``` Notebook that is contained in the same folder as this Notebook.\n",
    "\n",
    "In particular, the following operations are going to be executed:\n",
    "* Opening of the CSV preprocessed dumps\n",
    "* Join between the two datasets\n",
    "* Drop of the useless columns\n",
    "* Fix of the mismatched data types\n",
    "\n",
    "Lastly, the entire preprocessed dump is going to be saved on disk in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "Please set your working directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PATHS ********************+\n",
    "\n",
    "# Dumps Directory Path\n",
    "path_file_import = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Import/COCI_RAW/'\n",
    "\n",
    "# CSV Exports Directory Path\n",
    "path_file_export = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine New Data with a \"Partial\" CSV\n",
    "\n",
    "This can be really useful in case of limited disk space, allowing us to partially process the dump (using a subset of the CSVs) and free some space on disk by deleting the CSVs that have been already processed.\n",
    "\n",
    "**Note**: the delete operations need to be made manually\n",
    "**Note**: the partial CSV needs to be in the same format of the one generated with this script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_with_partial_csv = True\n",
    "partial_csv_path = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Import/COCI_PARTIAL/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read of the DBLP + MAG CSV Joined Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_with_partial_csv:\n",
    "    new_df_joined_partial = pd.read_csv(partial_csv_path + 'out_citations_by_year_and_conferences.csv', low_memory=False, index_col=[0])\n",
    "    print(f'Successfully Imported the Partial CSV')\n",
    "\n",
    "df_joined = pd.read_csv(path_file_export + 'out_dblp_and_mag_joined.csv', low_memory=False, index_col=[0])\n",
    "print(f'Successfully Imported the DBLP + MAG CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Support Dataframe\n",
    "It's going to help us extracting the citation' year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of the useless mag citations column\n",
    "df_joined = df_joined.drop(columns=['CitationCount_Mag', 'CitationCount_MagEstimated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the columns that are going to contain the citation obtained by a paper during a specific year. Also, needed for filtering the COCI paper that are not contained neither and MAG or DBLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_support_empty = df_joined.copy()\n",
    "\n",
    "# Drop of the useless column\n",
    "df_support_empty = df_support_empty.drop(columns=['ConferenceLocation', 'ConferenceNormalizedName', 'ConferenceTitle', 'OriginalTitle'])\n",
    "\n",
    "# Creation of the support column\n",
    "df_support_empty['Year_of_Citation'] = np.nan\n",
    "df_support_empty.rename(columns={'Year': 'Year_of_Publication'}, inplace=True)\n",
    "df_support_empty = df_support_empty.reindex(sorted(df_support_empty.columns), axis=1)\n",
    "\n",
    "df_support_empty.loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Year Citation Columns to the Original Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1950 # Probably there aren't citations before this date. We'll drop the empty columns later\n",
    "actual_year = date.today().year\n",
    "\n",
    "if not combine_with_partial_csv:\n",
    "    for i in range(start_year, actual_year + 1):\n",
    "        df_joined[str(i)] = 0\n",
    "else:\n",
    "    # We're going to use the partial joined dataframe\n",
    "    # The original dataframe was only needed for the creation of the support dataframe structure\n",
    "    df_joined = new_df_joined_partial.copy()\n",
    "    new_df_joined_partial = None\n",
    "\n",
    "df_joined.loc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Join of the COCI Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All Files' Names\n",
    "coci_all_csvs = glob.glob(path_file_import + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "tot_csvs = coci_all_csvs.__len__()\n",
    "\n",
    "for current_csv_name in coci_all_csvs:\n",
    "\n",
    "    # Empty the support dataframe\n",
    "    df_support = df_support_empty.copy()\n",
    "\n",
    "    # Open the current CSV\n",
    "    print(f'Currently processing CSV {count} ({tot_csvs} total): {current_csv_name}')\n",
    "    count += 1\n",
    "    df_coci_current_csv = pd.read_csv(current_csv_name, low_memory=False)\n",
    "\n",
    "    # Drop of the useless columns: 'oci', 'citing', 'creation', 'journal_sc', 'author_sc'\n",
    "    df_coci_current_csv = df_coci_current_csv.drop(columns=['oci', 'citing', 'creation', 'journal_sc', 'author_sc'])\n",
    "\n",
    "    # Column rename\n",
    "    df_coci_current_csv = df_coci_current_csv.rename(columns={'cited': 'Doi'})\n",
    "\n",
    "    # Making sure that everything has the same format\n",
    "    df_coci_current_csv.Doi = df_coci_current_csv.Doi.str.lower()\n",
    "\n",
    "    # Join with the support dataframe\n",
    "    df_support = pd.merge(df_support, df_coci_current_csv, on=['Doi'], how='inner')\n",
    "\n",
    "    # Filtering the rows with a negative timespan\n",
    "    df_support.timespan = df_support[\"timespan\"].astype(str)\n",
    "    df_support = df_support[~df_support[\"timespan\"].str.contains('-')]\n",
    "\n",
    "    # Computing the citation's year\n",
    "    df_support.Year_of_Citation = df_support.timespan.str.split('Y').str[0].str.split('P').str[1]\n",
    "    df_support = df_support.dropna(subset=['Year_of_Citation']) # Drop of the broken records\n",
    "    df_support.Year_of_Citation = df_support.Year_of_Citation.astype(int) + df_support.Year_of_Publication.astype(int)\n",
    "\n",
    "    # Removing the broken records\n",
    "    df_support = df_support.loc[(df_support['Year_of_Citation'] <= actual_year)] # Keeping only year <= actual year\n",
    "    df_support = df_support.loc[(df_support['Year_of_Citation'] >= start_year)] # Keeping only year >= 1950\n",
    "\n",
    "    # Reshaping the dataframe and resetting its index\n",
    "    df_support_reshaped = pd.crosstab(df_support.Doi, df_support.Year_of_Citation)\n",
    "    df_support_reshaped = df_support_reshaped.reset_index()\n",
    "\n",
    "    # Fixing the column name type\n",
    "    for column in df_support_reshaped:\n",
    "        df_support_reshaped.rename(columns = {column: str(column)}, inplace=True)\n",
    "\n",
    "    # Join with the original dataframe\n",
    "    df_joined = pd.merge(df_joined, df_support_reshaped, on=['Doi'], how='left')\n",
    "\n",
    "    # Sum of the citation counts values\n",
    "    for column in df_joined:\n",
    "        if '_x' in str(column):\n",
    "            coci_column = str(column).split('_x')[0] + '_y'\n",
    "\n",
    "            # Replacing nan with zeros in the coci rows that didn't match\n",
    "            df_joined[coci_column] = df_joined[coci_column].fillna(0).astype(int)\n",
    "\n",
    "            # Column sum\n",
    "            df_joined[column] += df_joined[coci_column]\n",
    "            \n",
    "            # Column rename and drop\n",
    "            df_joined.rename(columns = {column: str(column).split('_x')[0]}, inplace=True)\n",
    "            df_joined = df_joined.drop(columns=[coci_column])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write of the Final CSV on Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the resulting dataframe on disk in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write of the resulting CSV on Disk\n",
    "df_joined.to_csv(path_file_export + 'out_citations_by_year_and_conferences.csv')\n",
    "print(f'Successfully Exported the Joined CSV to {path_file_export}out_citations_by_year_and_conferences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check of the Exported CSV to be sure that everything went fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of the Exported CSV\n",
    "df_joined_exported_csv = pd.read_csv(path_file_export + 'out_citations_by_year_and_conferences.csv', low_memory=False, index_col=[0])\n",
    "df_joined_exported_csv"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
