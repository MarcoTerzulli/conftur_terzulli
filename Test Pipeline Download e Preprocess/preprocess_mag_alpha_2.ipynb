{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************************\n",
    "# Preprocess Microsoft Academics Graph (MAG) Dataset\n",
    "# ***************************************************************\n",
    "\n",
    "# Jupyter Notebook for the preprocessing of the Microsoft Academics Graph (MAG) dump\n",
    "#\n",
    "# TODO **********\n",
    "# In particular, the following operations are going to be executed:\n",
    "# - Opening of ConferenceInstances and ConferenceSeries CSVs\n",
    "# - Drop of the useless columns \n",
    "# - Merge of the two CSVs on the ConferenceSeriesID column\n",
    "# - Chuncked Processing of the Papers CSV\n",
    "# ---- Drop of the useless columns\n",
    "# ---- Drop of papers from journals and books rows\n",
    "# - Merge with the processed conferences data\n",
    "#\n",
    "# Lastly, the entire preprocessed dump is going to be saved on disk in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PATHS ********************+\n",
    "\n",
    "# Dumps Directory Path\n",
    "path_file_import = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/MAG/'\n",
    "\n",
    "# CSV Exports Directory Path\n",
    "path_file_export = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/MAG_Chunks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE INSTANCES ********************\n",
    "\n",
    "# Read of the Conference Instances File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_instances_col_names = ['ConferenceInstanceID', 'NormalizedName', 'DisplayName', 'ConferenceSeriesID', 'Location', 'OfficialUrl', 'StartDate', 'EndDate', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_instances = pd.read_csv(path_file_import + 'ConferenceInstances.txt', sep='\\t', names=df_mag_conf_instances_col_names)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Instances' Useless Columns\n",
    "df_mag_conf_instances = df_mag_conf_instances.drop(columns=['OfficialUrl', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate', 'StartDate', 'EndDate'])\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_instances.rename(columns={'NormalizedName': 'ConferenceNormalizedName', 'DisplayName': 'ConferenceDisplayName', 'Location': 'ConferenceLocation'}, inplace=True)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE SERIES ********************\n",
    "\n",
    "# Read of the Conference Series File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_series_col_names = ['ConferenceSeriesID', 'Rank', 'NormalizedName', 'DisplayName', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_series = pd.read_csv(path_file_import + 'ConferenceSeries.txt', sep='\\t', names=df_mag_conf_series_col_names)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Series' Useless Columns\n",
    "df_mag_conf_series = df_mag_conf_series.drop(columns=['Rank', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate'])\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_series.rename(columns={'NormalizedName': 'ConferenceSeriesNormalizedName', 'DisplayName': 'ConferenceSeriesDisplayName'}, inplace=True)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* MERGE OF CONFERENCE DATA ********************\n",
    "\n",
    "# Merge of the conference series and conference instances dataframes over the conferenceseriesid column\n",
    "df_mag_conf_merged = df_mag_conf_instances.merge(df_mag_conf_series, on='ConferenceSeriesID')\n",
    "df_mag_conf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PAPERS ********************\n",
    "\n",
    "# The Papers CSV is going to be processed in chunks, due to its size\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_papers_col_names = ['PaperID', 'Rank', 'Doi', 'DocType', 'PaperTitle', 'OriginalTitle', 'BookTitle', 'Year', 'Date', 'OnlineDate', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'ConferenceInstanceID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'EstimatedCitation', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate']\n",
    "\n",
    "# List of processed chunks.\n",
    "df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "# Define of the chunk size\n",
    "chunksize = 10 ** 7\n",
    "\n",
    "count = 1\n",
    "with pd.read_csv(path_file_import + 'Papers.txt', sep='\\t', chunksize=chunksize, low_memory=False, on_bad_lines='skip', names=df_mag_papers_col_names) as reader:\n",
    "    for chunk in reader:\n",
    "\n",
    "        # Drop of the useless columns\n",
    "        chunk = chunk.drop(columns=['Rank', 'OnlineDate', 'Publisher', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate', 'JournalID', 'BookTitle'])\n",
    "\n",
    "        # Filtering of papers without DOI\n",
    "        chunk = chunk.dropna(subset = ['Doi'])\n",
    "\n",
    "        # Filtering papers that are not related to conferences\n",
    "        chunk = chunk[chunk.DocType == 'Conference']\n",
    "\n",
    "        # Drop of the doctype column\n",
    "        chunk = chunk.drop(columns=['DocType'])\n",
    "\n",
    "        # Writing the partial CSV on Disk\n",
    "        chunk.to_csv(path_file_export + 'out_mag_citations_chunk_'+ str(count) +'.csv')\n",
    "\n",
    "        print(f'Successfully processed chunk {count} out of around {200000000 / chunksize}')\n",
    "        count += 1\n",
    "\n",
    "# Concatenation of the processed chunks\n",
    "#df_mag_papers = pd.concat(df_mag_papers_list_of_chunks)\n",
    "\n",
    "# Empty the list to free some memory\n",
    "#df_mag_papers_list_of_chunks = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PAPERS ********************\n",
    "\n",
    "# The Papers CSV is going to be processed in chunks, due to its size\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_papers_col_names = ['PaperID', 'Rank', 'Doi', 'DocType', 'PaperTitle', 'OriginalTitle', 'BookTitle', 'Year', 'Date', 'OnlineDate', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'ConferenceInstanceID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'EstimatedCitation', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'CreatedDate']\n",
    "\n",
    "# List of processed chunks.\n",
    "df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "# Define of the chunk size\n",
    "chunksize = 10 ** 7\n",
    "\n",
    "count = 1\n",
    "with pd.read_csv(path_file_import + 'Papers.txt', sep='\\t', names=df_mag_papers_col_names, chunksize=chunksize, low_memory=False, on_bad_lines='skip') as reader:\n",
    "    for chunk in reader:\n",
    "        print(f'Currently processing chunk {count} out of around 400')\n",
    "        count += 1\n",
    "\n",
    "        # Drop of the useless columns\n",
    "        chunk = chunk.drop(columns=['Rank', 'OnlineDate', 'Publisher', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'CreatedDate'])\n",
    "\n",
    "        # Filtering of books and Journals papers\n",
    "        # TODO\n",
    "\n",
    "        # Insert of the resulting chunk in the list \n",
    "        df_mag_papers_list_of_chunks.append(chunk)\n",
    "\n",
    "# Concatenation of the processed chunks\n",
    "df_mag_papers = pd.concat(df_mag_papers_list_of_chunks)\n",
    "\n",
    "# Empty the list to free some memory\n",
    "df_mag_papers_list_of_chunks = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export of the partial dataframe\n",
    "df_mag_papers.to_csv(path_file_export + 'out_mag_citations_partial.csv')\n",
    "print(f'Successfully Exported the Partial Preprocessed CSV to {path_file_export}out_mag_citations_partial.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All Files' Names\n",
    "coci_all_csvs = glob.glob(path_file_import + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coci_processed = pd.DataFrame(columns=['article', 'citations_count'])\n",
    "\n",
    "# Combine new data with a partial CSV\n",
    "if combine_with_partial_csv:\n",
    "    df_coci_processed = pd.read_csv(partial_csv_path, low_memory=False)\n",
    "    print(f'Successfully Imported the Partial CSV')\n",
    "    \n",
    "# Read, process and concat all CSVs\n",
    "count = 0\n",
    "for current_csv_name in coci_all_csvs:\n",
    "\n",
    "    # Open the current CSV\n",
    "    print(f'Currently processing CSV {count}: {current_csv_name}')\n",
    "    count += 1\n",
    "    df_coci_current_csv = pd.read_csv(current_csv_name, low_memory=False)\n",
    "\n",
    "    # Drop of the useless columns: 'oci', 'citing', 'creation', 'timespan', 'journal_sc', 'author_sc'\n",
    "    df_coci_current_csv.drop(columns=['oci', 'citing', 'creation', 'timespan', 'journal_sc', 'author_sc'])\n",
    "\n",
    "    # Group by cited article and count\n",
    "    sf_coci_current_grouped = df_coci_current_csv.groupby(['cited'])['cited'].count()\n",
    "\n",
    "    # Since the returned object is a Pandas Series type, we need to convert it to a Pandas dataframe\n",
    "    df_coci_current_csv = pd.DataFrame({'article':sf_coci_current_grouped.index, 'citations_count':sf_coci_current_grouped.values})\n",
    "\n",
    "    ### Concat with the data previously elaborated\n",
    "    df_coci_processed = pd.concat([df_coci_processed, df_coci_current_csv])\n",
    "\n",
    "    # Now we need to do a new group by and sum the citations_count to reduce the data\n",
    "    sf_coci_processed_grouped = df_coci_processed.groupby(['article'])['citations_count'].sum()\n",
    "    df_coci_processed = pd.DataFrame({'article':sf_coci_processed_grouped.index, 'citations_count':sf_coci_processed_grouped.values})\n",
    "\n",
    "# Export of the final dataframe\n",
    "df_coci_processed.to_csv(path_file_export + 'out_coci_citations_count.csv')\n",
    "print(f'Successfully Exported the Preprocessed CSV to {path_file_export}out_coci_citations_count.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of the Exported CSV\n",
    "df_coci_exported_csv = pd.read_csv(path_file_export + 'out_coci_citations_count.csv', low_memory=False)\n",
    "df_coci_exported_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by citations count descending to see the articles with the most citations\n",
    "df_coci_exported_csv = df_coci_exported_csv.sort_values(by='citations_count', ascending=False)\n",
    "df_coci_exported_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the total count of the citations contained in the extracted CSV\n",
    "df_coci_exported_csv['citations_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
