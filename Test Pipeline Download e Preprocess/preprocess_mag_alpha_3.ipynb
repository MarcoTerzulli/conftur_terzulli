{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************************\n",
    "# Preprocess Microsoft Academics Graph (MAG) Dataset\n",
    "# ***************************************************************\n",
    "\n",
    "# Jupyter Notebook for the preprocessing of the Microsoft Academics Graph (MAG) dump\n",
    "#\n",
    "# TODO **********\n",
    "# In particular, the following operations are going to be executed:\n",
    "# - Opening of ConferenceInstances and ConferenceSeries CSVs\n",
    "# - Drop of the useless columns \n",
    "# - Merge of the two CSVs on the ConferenceSeriesID column\n",
    "# - Chuncked Processing of the Papers CSV\n",
    "# ---- Drop of the useless columns\n",
    "# ---- Drop of papers from journals and books rows\n",
    "# - Merge with the processed conferences data\n",
    "#\n",
    "# Lastly, the entire preprocessed dump is going to be saved on disk in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PATHS ********************+\n",
    "\n",
    "# Dumps Directory Path\n",
    "path_file_import = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/MAG/'\n",
    "\n",
    "# CSV Exports Directory Path\n",
    "path_file_export = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/MAG_Chunks/'\n",
    "\n",
    "# Use a Previuously Preprocessed Papers CSV\n",
    "#\n",
    "# This can be really useful to save some time using a previously elaborated\n",
    "# CSV file. We don't need to repeat the same operations!\n",
    "#\n",
    "# Note: the CSV needs to be in the same format of the one generated with this script\n",
    "read_preprocessed_papers = True\n",
    "preprocessed_papers_csv_path = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/out_mag_papers.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE INSTANCES ********************\n",
    "\n",
    "# Read of the Conference Instances File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_instances_col_names = ['ConferenceInstanceID', 'NormalizedName', 'DisplayName', 'ConferenceSeriesID', 'Location', 'OfficialUrl', 'StartDate', 'EndDate', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_instances = pd.read_csv(path_file_import + 'ConferenceInstances.txt', sep='\\t', names=df_mag_conf_instances_col_names)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Instances' Useless Columns\n",
    "df_mag_conf_instances = df_mag_conf_instances.drop(columns=['OfficialUrl', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate', 'StartDate', 'EndDate'])\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_instances.rename(columns={'NormalizedName': 'ConferenceNormalizedName', 'DisplayName': 'ConferenceDisplayName', 'Location': 'ConferenceLocation'}, inplace=True)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE SERIES ********************\n",
    "\n",
    "# Read of the Conference Series File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_series_col_names = ['ConferenceSeriesID', 'Rank', 'NormalizedName', 'DisplayName', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_series = pd.read_csv(path_file_import + 'ConferenceSeries.txt', sep='\\t', names=df_mag_conf_series_col_names)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Series' Useless Columns\n",
    "df_mag_conf_series = df_mag_conf_series.drop(columns=['Rank', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate'])\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_series.rename(columns={'NormalizedName': 'ConferenceSeriesNormalizedName', 'DisplayName': 'ConferenceSeriesDisplayName'}, inplace=True)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* MERGE OF CONFERENCE DATA ********************\n",
    "\n",
    "# Merge of the conference series and conference instances dataframes over the conferenceseriesid column\n",
    "df_mag_conf_merged = df_mag_conf_instances.merge(df_mag_conf_series, on='ConferenceSeriesID')\n",
    "df_mag_conf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PAPERS ********************\n",
    "\n",
    "# Read of previously prerocessed CSV\n",
    "df_mag_papers = None\n",
    "if read_preprocessed_papers:\n",
    "    df_mag_papers = pd.read_csv(preprocessed_papers_csv_path, low_memory=False, index_col=0)\n",
    "else:\n",
    "    # The Papers CSV is going to be processed in chunks, due to its size\n",
    "\n",
    "    # The column names follow the MAG' scheme official documentation\n",
    "    df_mag_papers_col_names = ['PaperID', 'Rank', 'Doi', 'DocType', 'PaperTitle', 'OriginalTitle', 'BookTitle', 'Year', 'Date', 'OnlineDate', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'ConferenceInstanceID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'EstimatedCitation', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate']\n",
    "\n",
    "    # List of processed chunks.\n",
    "    df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "    # Define of the chunk size\n",
    "    chunksize = 10 ** 7\n",
    "\n",
    "    count = 1\n",
    "    with pd.read_csv(path_file_import + 'Papers.txt', sep='\\t', chunksize=chunksize, low_memory=False, on_bad_lines='skip', names=df_mag_papers_col_names) as reader:\n",
    "        for chunk in reader:\n",
    "\n",
    "            # Drop of the useless columns\n",
    "            chunk = chunk.drop(columns=['Rank', 'OnlineDate', 'Publisher', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate', 'JournalID', 'BookTitle', 'Date'])\n",
    "\n",
    "            # Filtering of papers without DOI\n",
    "            chunk = chunk.dropna(subset = ['Doi'])\n",
    "\n",
    "            # Filtering papers that are not related to conferences\n",
    "            chunk = chunk[chunk.DocType == 'Conference']\n",
    "\n",
    "            # Drop of the doctype column\n",
    "            chunk = chunk.drop(columns=['DocType'])\n",
    "\n",
    "            # Insert of the resulting chunk in the list \n",
    "            df_mag_papers_list_of_chunks.append(chunk)\n",
    "\n",
    "            print(f'Successfully processed chunk {count} out of around {260000000 / chunksize}')\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "    # Concatenation of the processed chunks\n",
    "    df_mag_papers = pd.concat(df_mag_papers_list_of_chunks)\n",
    "\n",
    "    # Empty the list to free some memory\n",
    "    df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "    # Write of the resulting CSV on Disk\n",
    "    df_mag_papers.to_csv(path_file_export + 'out_mag_papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PaperID</th>\n",
       "      <th>Doi</th>\n",
       "      <th>PaperTitle</th>\n",
       "      <th>OriginalTitle</th>\n",
       "      <th>Year</th>\n",
       "      <th>ConferenceSeriesID</th>\n",
       "      <th>ConferenceInstanceID</th>\n",
       "      <th>CitationCount</th>\n",
       "      <th>EstimatedCitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14558443</td>\n",
       "      <td>10.1007/978-3-662-45174-8_28</td>\n",
       "      <td>the adaptive priority queue with elimination a...</td>\n",
       "      <td>The Adaptive Priority Queue with Elimination a...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.131603e+09</td>\n",
       "      <td>4038532.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15354235</td>\n",
       "      <td>10.1007/978-3-662-44777-2_60</td>\n",
       "      <td>document retrieval on repetitive collections</td>\n",
       "      <td>Document Retrieval on Repetitive Collections</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.154039e+09</td>\n",
       "      <td>157008481.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>24327294</td>\n",
       "      <td>10.1007/978-3-319-03973-2_13</td>\n",
       "      <td>socomo marketing for travel and tourism</td>\n",
       "      <td>SoCoMo Marketing for Travel and Tourism</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.196984e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>60437532</td>\n",
       "      <td>10.1007/3-540-46146-9_77</td>\n",
       "      <td>similarity image retrieval system using hierar...</td>\n",
       "      <td>Similarity Image Retrieval System Using Hierar...</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1.192665e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>198056957</td>\n",
       "      <td>10.1007/11785231_94</td>\n",
       "      <td>leukemia prediction from gene expression data ...</td>\n",
       "      <td>Leukemia prediction from gene expression data—...</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>1.176896e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259718386</th>\n",
       "      <td>3102242761</td>\n",
       "      <td>10.1109/IECON43393.2020.9254316</td>\n",
       "      <td>loss reduction by synchronous rectification in...</td>\n",
       "      <td>Loss Reduction by Synchronous Rectification in...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2.623572e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259718500</th>\n",
       "      <td>3136855299</td>\n",
       "      <td>10.1109/BMSB49480.2020.9379806</td>\n",
       "      <td>data over cable services improving the bicm ca...</td>\n",
       "      <td>Data Over Cable Services – Improving the BICM ...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2.623662e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259718537</th>\n",
       "      <td>3145351916</td>\n",
       "      <td>10.1109/ACC.1988.4172843</td>\n",
       "      <td>model reference robust adaptive control withou...</td>\n",
       "      <td>Model Reference Robust Adaptive Control withou...</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>2.238538e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259718570</th>\n",
       "      <td>3151696876</td>\n",
       "      <td>10.1109/ICASSP.2002.1005676</td>\n",
       "      <td>missing data speech recognition in reverberant...</td>\n",
       "      <td>Missing data speech recognition in reverberant...</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1.121228e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259718611</th>\n",
       "      <td>3162646375</td>\n",
       "      <td>10.1109/ITCA52113.2020.00077</td>\n",
       "      <td>research on text to image based on generative ...</td>\n",
       "      <td>Research on Text to Image Based on Generative ...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2.622834e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4409816 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              PaperID                              Doi  \\\n",
       "37           14558443     10.1007/978-3-662-45174-8_28   \n",
       "39           15354235     10.1007/978-3-662-44777-2_60   \n",
       "68           24327294     10.1007/978-3-319-03973-2_13   \n",
       "197          60437532         10.1007/3-540-46146-9_77   \n",
       "666         198056957              10.1007/11785231_94   \n",
       "...               ...                              ...   \n",
       "259718386  3102242761  10.1109/IECON43393.2020.9254316   \n",
       "259718500  3136855299   10.1109/BMSB49480.2020.9379806   \n",
       "259718537  3145351916         10.1109/ACC.1988.4172843   \n",
       "259718570  3151696876      10.1109/ICASSP.2002.1005676   \n",
       "259718611  3162646375     10.1109/ITCA52113.2020.00077   \n",
       "\n",
       "                                                  PaperTitle  \\\n",
       "37         the adaptive priority queue with elimination a...   \n",
       "39              document retrieval on repetitive collections   \n",
       "68                   socomo marketing for travel and tourism   \n",
       "197        similarity image retrieval system using hierar...   \n",
       "666        leukemia prediction from gene expression data ...   \n",
       "...                                                      ...   \n",
       "259718386  loss reduction by synchronous rectification in...   \n",
       "259718500  data over cable services improving the bicm ca...   \n",
       "259718537  model reference robust adaptive control withou...   \n",
       "259718570  missing data speech recognition in reverberant...   \n",
       "259718611  research on text to image based on generative ...   \n",
       "\n",
       "                                               OriginalTitle    Year  \\\n",
       "37         The Adaptive Priority Queue with Elimination a...  2014.0   \n",
       "39              Document Retrieval on Repetitive Collections  2014.0   \n",
       "68                   SoCoMo Marketing for Travel and Tourism  2013.0   \n",
       "197        Similarity Image Retrieval System Using Hierar...  2002.0   \n",
       "666        Leukemia prediction from gene expression data—...  2006.0   \n",
       "...                                                      ...     ...   \n",
       "259718386  Loss Reduction by Synchronous Rectification in...  2020.0   \n",
       "259718500  Data Over Cable Services – Improving the BICM ...  2020.0   \n",
       "259718537  Model Reference Robust Adaptive Control withou...  1988.0   \n",
       "259718570  Missing data speech recognition in reverberant...  2002.0   \n",
       "259718611  Research on Text to Image Based on Generative ...  2020.0   \n",
       "\n",
       "           ConferenceSeriesID  ConferenceInstanceID  CitationCount  \\\n",
       "37               1.131603e+09             4038532.0           12.0   \n",
       "39               1.154039e+09           157008481.0           10.0   \n",
       "68               1.196984e+09                   NaN           20.0   \n",
       "197              1.192665e+09                   NaN            0.0   \n",
       "666              1.176896e+09                   NaN           19.0   \n",
       "...                       ...                   ...            ...   \n",
       "259718386        2.623572e+09                   NaN            0.0   \n",
       "259718500        2.623662e+09                   NaN            0.0   \n",
       "259718537        2.238538e+09                   NaN            0.0   \n",
       "259718570        1.121228e+09                   NaN            0.0   \n",
       "259718611        2.622834e+09                   NaN            0.0   \n",
       "\n",
       "          EstimatedCitation  \n",
       "37                       12  \n",
       "39                       10  \n",
       "68                       20  \n",
       "197                       0  \n",
       "666                      19  \n",
       "...                     ...  \n",
       "259718386                 0  \n",
       "259718500                 0  \n",
       "259718537                 0  \n",
       "259718570                 0  \n",
       "259718611                 0  \n",
       "\n",
       "[4409816 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mag_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PaperID</th>\n",
       "      <th>Doi</th>\n",
       "      <th>PaperTitle</th>\n",
       "      <th>OriginalTitle</th>\n",
       "      <th>Year</th>\n",
       "      <th>ConferenceSeriesID</th>\n",
       "      <th>ConferenceInstanceID</th>\n",
       "      <th>CitationCount</th>\n",
       "      <th>EstimatedCitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14558443</td>\n",
       "      <td>10.1007/978-3-662-45174-8_28</td>\n",
       "      <td>the adaptive priority queue with elimination a...</td>\n",
       "      <td>The Adaptive Priority Queue with Elimination a...</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.131603e+09</td>\n",
       "      <td>4.038532e+06</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15354235</td>\n",
       "      <td>10.1007/978-3-662-44777-2_60</td>\n",
       "      <td>document retrieval on repetitive collections</td>\n",
       "      <td>Document Retrieval on Repetitive Collections</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.154039e+09</td>\n",
       "      <td>1.570085e+08</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>1459710595</td>\n",
       "      <td>10.1007/978-3-319-08958-4_17</td>\n",
       "      <td>enhancing labeled data using unlabeled data fo...</td>\n",
       "      <td>Enhancing Labeled Data Using Unlabeled Data fo...</td>\n",
       "      <td>2011</td>\n",
       "      <td>2.760407e+09</td>\n",
       "      <td>2.019264e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>2161259116</td>\n",
       "      <td>10.1109/CVPR.2013.65</td>\n",
       "      <td>improved image set classification via joint sp...</td>\n",
       "      <td>Improved Image Set Classification via Joint Sp...</td>\n",
       "      <td>2013</td>\n",
       "      <td>1.158168e+09</td>\n",
       "      <td>1.383691e+08</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>1488332647</td>\n",
       "      <td>10.1109/ISBMSB.2008.4536671</td>\n",
       "      <td>a novel channel estimation based on spread pil...</td>\n",
       "      <td>A novel channel estimation based on spread pil...</td>\n",
       "      <td>2008</td>\n",
       "      <td>2.623662e+09</td>\n",
       "      <td>2.626942e+09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259717113</th>\n",
       "      <td>2789624042</td>\n",
       "      <td>10.1109/ISSCC.2018.8310368</td>\n",
       "      <td>a 95 2 efficiency dual path dc dc step up conv...</td>\n",
       "      <td>A 95.2% efficiency dual-path DC-DC step-up con...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.183230e+09</td>\n",
       "      <td>2.788559e+09</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259717694</th>\n",
       "      <td>2893506456</td>\n",
       "      <td>10.1007/978-3-030-01418-6_54</td>\n",
       "      <td>learning preferences for large scale multi lab...</td>\n",
       "      <td>Learning Preferences for Large Scale Multi-lab...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.158833e+09</td>\n",
       "      <td>2.892231e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259717766</th>\n",
       "      <td>2914533952</td>\n",
       "      <td>10.1145/3308558.3313726</td>\n",
       "      <td>city wide signal strength maps prediction with...</td>\n",
       "      <td>City-Wide Signal Strength Maps: Prediction wit...</td>\n",
       "      <td>2019</td>\n",
       "      <td>1.135342e+09</td>\n",
       "      <td>2.890478e+09</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259717787</th>\n",
       "      <td>2921629601</td>\n",
       "      <td>10.1109/ICOSC.2019.8665672</td>\n",
       "      <td>constructing and maintaining corpus driven ann...</td>\n",
       "      <td>Constructing and Maintaining Corpus-Driven Ann...</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.898614e+09</td>\n",
       "      <td>2.889800e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259717991</th>\n",
       "      <td>2967304537</td>\n",
       "      <td>10.1109/ICRA.2019.8794157</td>\n",
       "      <td>1 actuator 3 dof manipulation using an underac...</td>\n",
       "      <td>1-Actuator 3-DoF Manipulation Using an Underac...</td>\n",
       "      <td>2019</td>\n",
       "      <td>1.163902e+09</td>\n",
       "      <td>2.891770e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1431517 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              PaperID                           Doi  \\\n",
       "37           14558443  10.1007/978-3-662-45174-8_28   \n",
       "39           15354235  10.1007/978-3-662-44777-2_60   \n",
       "2181       1459710595  10.1007/978-3-319-08958-4_17   \n",
       "2192       2161259116          10.1109/CVPR.2013.65   \n",
       "2523       1488332647   10.1109/ISBMSB.2008.4536671   \n",
       "...               ...                           ...   \n",
       "259717113  2789624042    10.1109/ISSCC.2018.8310368   \n",
       "259717694  2893506456  10.1007/978-3-030-01418-6_54   \n",
       "259717766  2914533952       10.1145/3308558.3313726   \n",
       "259717787  2921629601    10.1109/ICOSC.2019.8665672   \n",
       "259717991  2967304537     10.1109/ICRA.2019.8794157   \n",
       "\n",
       "                                                  PaperTitle  \\\n",
       "37         the adaptive priority queue with elimination a...   \n",
       "39              document retrieval on repetitive collections   \n",
       "2181       enhancing labeled data using unlabeled data fo...   \n",
       "2192       improved image set classification via joint sp...   \n",
       "2523       a novel channel estimation based on spread pil...   \n",
       "...                                                      ...   \n",
       "259717113  a 95 2 efficiency dual path dc dc step up conv...   \n",
       "259717694  learning preferences for large scale multi lab...   \n",
       "259717766  city wide signal strength maps prediction with...   \n",
       "259717787  constructing and maintaining corpus driven ann...   \n",
       "259717991  1 actuator 3 dof manipulation using an underac...   \n",
       "\n",
       "                                               OriginalTitle  Year  \\\n",
       "37         The Adaptive Priority Queue with Elimination a...  2014   \n",
       "39              Document Retrieval on Repetitive Collections  2014   \n",
       "2181       Enhancing Labeled Data Using Unlabeled Data fo...  2011   \n",
       "2192       Improved Image Set Classification via Joint Sp...  2013   \n",
       "2523       A novel channel estimation based on spread pil...  2008   \n",
       "...                                                      ...   ...   \n",
       "259717113  A 95.2% efficiency dual-path DC-DC step-up con...  2018   \n",
       "259717694  Learning Preferences for Large Scale Multi-lab...  2018   \n",
       "259717766  City-Wide Signal Strength Maps: Prediction wit...  2019   \n",
       "259717787  Constructing and Maintaining Corpus-Driven Ann...  2019   \n",
       "259717991  1-Actuator 3-DoF Manipulation Using an Underac...  2019   \n",
       "\n",
       "           ConferenceSeriesID  ConferenceInstanceID  CitationCount  \\\n",
       "37               1.131603e+09          4.038532e+06           12.0   \n",
       "39               1.154039e+09          1.570085e+08           10.0   \n",
       "2181             2.760407e+09          2.019264e+08            0.0   \n",
       "2192             1.158168e+09          1.383691e+08           75.0   \n",
       "2523             2.623662e+09          2.626942e+09            4.0   \n",
       "...                       ...                   ...            ...   \n",
       "259717113        1.183230e+09          2.788559e+09            8.0   \n",
       "259717694        1.158833e+09          2.892231e+09            1.0   \n",
       "259717766        1.135342e+09          2.890478e+09            8.0   \n",
       "259717787        2.898614e+09          2.889800e+09            1.0   \n",
       "259717991        1.163902e+09          2.891770e+09            0.0   \n",
       "\n",
       "          EstimatedCitation  \n",
       "37                       12  \n",
       "39                       10  \n",
       "2181                      0  \n",
       "2192                     95  \n",
       "2523                      4  \n",
       "...                     ...  \n",
       "259717113                 8  \n",
       "259717694                 1  \n",
       "259717766                 8  \n",
       "259717787                 1  \n",
       "259717991                 0  \n",
       "\n",
       "[1431517 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Filtering of papers without DOI\n",
    "df_mag_papers_cp = df_mag_papers\n",
    "df_mag_papers_cp = df_mag_papers_cp.dropna(subset = ['ConferenceInstanceID'])\n",
    "df_mag_papers_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PAPERS ********************\n",
    "\n",
    "# The Papers CSV is going to be processed in chunks, due to its size\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_papers_col_names = ['PaperID', 'Rank', 'Doi', 'DocType', 'PaperTitle', 'OriginalTitle', 'BookTitle', 'Year', 'Date', 'OnlineDate', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'ConferenceInstanceID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'EstimatedCitation', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'CreatedDate']\n",
    "\n",
    "# List of processed chunks.\n",
    "df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "# Define of the chunk size\n",
    "chunksize = 10 ** 7\n",
    "\n",
    "count = 1\n",
    "with pd.read_csv(path_file_import + 'Papers.txt', sep='\\t', names=df_mag_papers_col_names, chunksize=chunksize, low_memory=False, on_bad_lines='skip') as reader:\n",
    "    for chunk in reader:\n",
    "        print(f'Currently processing chunk {count} out of around 400')\n",
    "        count += 1\n",
    "\n",
    "        # Drop of the useless columns\n",
    "        chunk = chunk.drop(columns=['Rank', 'OnlineDate', 'Publisher', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'CreatedDate'])\n",
    "\n",
    "        # Filtering of books and Journals papers\n",
    "        # TODO\n",
    "\n",
    "        # Insert of the resulting chunk in the list \n",
    "        df_mag_papers_list_of_chunks.append(chunk)\n",
    "\n",
    "# Concatenation of the processed chunks\n",
    "df_mag_papers = pd.concat(df_mag_papers_list_of_chunks)\n",
    "\n",
    "# Empty the list to free some memory\n",
    "df_mag_papers_list_of_chunks = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export of the partial dataframe\n",
    "df_mag_papers.to_csv(path_file_export + 'out_mag_citations_partial.csv')\n",
    "print(f'Successfully Exported the Partial Preprocessed CSV to {path_file_export}out_mag_citations_partial.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All Files' Names\n",
    "coci_all_csvs = glob.glob(path_file_import + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coci_processed = pd.DataFrame(columns=['article', 'citations_count'])\n",
    "\n",
    "# Combine new data with a partial CSV\n",
    "if combine_with_partial_csv:\n",
    "    df_coci_processed = pd.read_csv(partial_csv_path, low_memory=False)\n",
    "    print(f'Successfully Imported the Partial CSV')\n",
    "    \n",
    "# Read, process and concat all CSVs\n",
    "count = 0\n",
    "for current_csv_name in coci_all_csvs:\n",
    "\n",
    "    # Open the current CSV\n",
    "    print(f'Currently processing CSV {count}: {current_csv_name}')\n",
    "    count += 1\n",
    "    df_coci_current_csv = pd.read_csv(current_csv_name, low_memory=False)\n",
    "\n",
    "    # Drop of the useless columns: 'oci', 'citing', 'creation', 'timespan', 'journal_sc', 'author_sc'\n",
    "    df_coci_current_csv.drop(columns=['oci', 'citing', 'creation', 'timespan', 'journal_sc', 'author_sc'])\n",
    "\n",
    "    # Group by cited article and count\n",
    "    sf_coci_current_grouped = df_coci_current_csv.groupby(['cited'])['cited'].count()\n",
    "\n",
    "    # Since the returned object is a Pandas Series type, we need to convert it to a Pandas dataframe\n",
    "    df_coci_current_csv = pd.DataFrame({'article':sf_coci_current_grouped.index, 'citations_count':sf_coci_current_grouped.values})\n",
    "\n",
    "    ### Concat with the data previously elaborated\n",
    "    df_coci_processed = pd.concat([df_coci_processed, df_coci_current_csv])\n",
    "\n",
    "    # Now we need to do a new group by and sum the citations_count to reduce the data\n",
    "    sf_coci_processed_grouped = df_coci_processed.groupby(['article'])['citations_count'].sum()\n",
    "    df_coci_processed = pd.DataFrame({'article':sf_coci_processed_grouped.index, 'citations_count':sf_coci_processed_grouped.values})\n",
    "\n",
    "# Export of the final dataframe\n",
    "df_coci_processed.to_csv(path_file_export + 'out_coci_citations_count.csv')\n",
    "print(f'Successfully Exported the Preprocessed CSV to {path_file_export}out_coci_citations_count.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of the Exported CSV\n",
    "df_coci_exported_csv = pd.read_csv(path_file_export + 'out_coci_citations_count.csv', low_memory=False)\n",
    "df_coci_exported_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by citations count descending to see the articles with the most citations\n",
    "df_coci_exported_csv = df_coci_exported_csv.sort_values(by='citations_count', ascending=False)\n",
    "df_coci_exported_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the total count of the citations contained in the extracted CSV\n",
    "df_coci_exported_csv['citations_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
