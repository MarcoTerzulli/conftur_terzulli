{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocess Microsoft Academics Graph (MAG) Dataset\n",
    "\n",
    "Jupyter Notebook for the preprocessing of the Microsoft Academics Graph (MAG) dump.\n",
    "\n",
    "For this process, the following CSV files are needed: ```ConferenceInstances.txt```, ```ConferenceSeries.txt```, ```Papers.txt```. \n",
    "The above files can be found here: https://archive.org/download/mag-2021-06-07/mag/\n",
    "\n",
    "In particular, the following operations are going to be executed:\n",
    "* Opening of ConferenceInstances and ConferenceSeries CSVs\n",
    "* Drop of the useless columns \n",
    "* Chuncked Processing of the Papers CSV\n",
    "    * Drop of the useless columns\n",
    "    * Drop of papers without DOI\n",
    "    * Drop of papers from journals and books rows\n",
    "* Merge with the processed conferences data\n",
    "\n",
    "Lastly, the entire preprocessed dump is going to be saved on disk in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import pandas as pd\n",
    "import platform\n",
    "import multiprocessing as mp \n",
    "import concurrent       \n",
    "from preprocess_multithread_utils import * \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "Please set your working directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PATHS ********************+\n",
    "\n",
    "# Dumps Directory Path\n",
    "path_file_import = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Import/'\n",
    "\n",
    "# CSV Exports Directory Path\n",
    "path_file_export = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Previuously Preprocessed Papers CSV\n",
    "This can be really useful to save some time using a previously elaborated CSV file. We don't need to repeat the same operations!\n",
    "\n",
    "**Note**: the CSV needs to be in the same format of the one generated with this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Previuously Preprocessed Papers CSV\n",
    "#\n",
    "# This can be really useful to save some time using a previously elaborated\n",
    "# CSV file. We don't need to repeat the same operations!\n",
    "#\n",
    "# Note: the CSV needs to be in the same format of the one generated with this script\n",
    "read_preprocessed_papers = True\n",
    "preprocessed_papers_csv_path = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/out_mag_papers.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading Settings\n",
    "Settings needed for the multithreaded queries to gather the missing conferences locations from the DBLP website.\n",
    "\n",
    "Please specify the number of CPU threads below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu_threads = 8 # Number of CPU threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special setting for the specific operating systems.\n",
    "\n",
    "**Note**: Due to the latest MacOS releases' security measures, we need to use the spawn method instead of fork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook running on Darwin OS: \n",
      "Spawn method has been set\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook running on {platform.system()} OS: \")\n",
    "\n",
    "if platform.system() == \"Darwin\" or platform.system() == \"Windows\": # MacOS and windows\n",
    "    mp_ctx = mp.get_context(\"spawn\")\n",
    "    print(\"Spawn method has been set\")\n",
    "    \n",
    "else: # other unix systems\n",
    "    mp_ctx = mp.get_context(\"fork\")\n",
    "    print(\"Spawn method has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess of Conference Instances CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE INSTANCES ********************\n",
    "\n",
    "# Read of the Conference Instances File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_instances_col_names = ['ConferenceInstanceID', 'NormalizedName', 'DisplayName', 'ConferenceSeriesID', 'Location', 'OfficialUrl', 'StartDate', 'EndDate', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_instances = pd.read_csv(path_file_import + 'ConferenceInstances.txt', sep='\\t', names=df_mag_conf_instances_col_names)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the useless columns are going to be removed from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Instances' Useless Columns\n",
    "df_mag_conf_instances = df_mag_conf_instances.drop(columns=['OfficialUrl', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate', 'StartDate', 'EndDate'])\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column rename to remove ambiguity for the future joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_instances.rename(columns={'NormalizedName': 'ConferenceNormalizedName', 'DisplayName': 'ConferenceDisplayName', 'Location': 'ConferenceLocation'}, inplace=True)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess of Conference Series CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE SERIES ********************\n",
    "\n",
    "# Read of the Conference Series File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_series_col_names = ['ConferenceSeriesID', 'Rank', 'NormalizedName', 'DisplayName', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_series = pd.read_csv(path_file_import + 'ConferenceSeries.txt', sep='\\t', names=df_mag_conf_series_col_names)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the useless columns are going to be removed from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Series' Useless Columns\n",
    "df_mag_conf_series = df_mag_conf_series.drop(columns=['Rank', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate'])\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column rename to remove ambiguity for the future joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_series.rename(columns={'NormalizedName': 'ConferenceSeriesNormalizedName', 'DisplayName': 'ConferenceSeriesDisplayName'}, inplace=True)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess of Papers CSV\n",
    "The Papers CSV is going to be processed in chunks, due to its size.\n",
    "\n",
    "The following operations are going to be executed:\n",
    "* Drop of the useless columns\n",
    "* Filtering of papers without DOI\n",
    "* Filtering papers that are not related to conferences\n",
    "* Drop of the doctype column\n",
    "* Write of the processed file on disk (in CSV format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PAPERS ********************\n",
    "\n",
    "# Read of previously prerocessed CSV\n",
    "df_mag_papers = None\n",
    "if read_preprocessed_papers:\n",
    "    df_mag_papers = pd.read_csv(preprocessed_papers_csv_path, low_memory=False, index_col=0)\n",
    "else:\n",
    "    # The Papers CSV is going to be processed in chunks, due to its size\n",
    "\n",
    "    # The column names follow the MAG' scheme official documentation\n",
    "    df_mag_papers_col_names = ['PaperID', 'Rank', 'Doi', 'DocType', 'PaperTitle', 'OriginalTitle', 'BookTitle', 'Year', 'Date', 'OnlineDate', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'ConferenceInstanceID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'EstimatedCitation', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate']\n",
    "\n",
    "    # List of processed chunks.\n",
    "    df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "    # Define of the chunk size\n",
    "    chunksize = 10 ** 7\n",
    "\n",
    "    count = 1\n",
    "    with pd.read_csv(path_file_import + 'Papers.txt', sep='\\t', chunksize=chunksize, low_memory=False, on_bad_lines='skip', names=df_mag_papers_col_names) as reader:\n",
    "        for chunk in reader:\n",
    "\n",
    "            # Drop of the useless columns\n",
    "            chunk = chunk.drop(columns=['Rank', 'OnlineDate', 'Publisher', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate', 'JournalID', 'BookTitle', 'Date'])\n",
    "\n",
    "            # Filtering of papers without DOI\n",
    "            chunk = chunk.dropna(subset = ['Doi'])\n",
    "\n",
    "            # Filtering papers that are not related to conferences\n",
    "            chunk = chunk[chunk.DocType == 'Conference']\n",
    "\n",
    "            # Drop of the doctype column\n",
    "            chunk = chunk.drop(columns=['DocType'])\n",
    "\n",
    "            # Insert of the resulting chunk in the list \n",
    "            df_mag_papers_list_of_chunks.append(chunk)\n",
    "\n",
    "            print(f'Successfully processed chunk {count} out of around {260000000 / chunksize}')\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "    # Concatenation of the processed chunks\n",
    "    df_mag_papers = pd.concat(df_mag_papers_list_of_chunks)\n",
    "\n",
    "    # Empty the list to free some memory\n",
    "    df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "    # Write of the resulting CSV on Disk\n",
    "    df_mag_papers.to_csv(path_file_export + 'out_mag_papers.csv')\n",
    "    print(f'Successfully Exported the Preprocessed Papers CSV to {path_file_export}out_mag_papers.csv')\n",
    "\n",
    "df_mag_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge of Conferences and Papers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* MERGE OF CONFERENCES AND PAPERS DATA ********************\n",
    "\n",
    "# Merge of conferences and papers data over the conferenceseries id columnn to get the conference series name\n",
    "# The papers' row that will not match will be preserved\n",
    "df_mag_preprocessed = pd.merge(df_mag_papers, df_mag_conf_series, on=['ConferenceSeriesID'], how='left')\n",
    "\n",
    "# Merge of conferences and papers data over the conferenceinstances id columnn\n",
    "# The papers' row that will not match will be preserved\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_mag_conf_instances, on=['ConferenceInstanceID'], how='left')\n",
    "\n",
    "# Drop of the duplicated columns\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceSeriesID_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceSeriesID_x':'ConferenceSeriesID'}, inplace=True)\n",
    "\n",
    "# Removing broken data (four records seems to have mismatched types in some columns)\n",
    "df_mag_preprocessed = df_mag_preprocessed.dropna(subset = ['CitationCount'])\n",
    "\n",
    "df_mag_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix of the Mismatched Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix of the year data type, that has been interpreted as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_mag_preprocessed.iloc[:1][\"Year\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed = df_mag_preprocessed.astype({\"Year\": int}, errors='raise') \n",
    "df_mag_preprocessed.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix of the CitationCount data type, that has been interpreted as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_mag_preprocessed.iloc[:1][\"CitationCount\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed = df_mag_preprocessed.astype({\"CitationCount\": int}, errors='raise') \n",
    "df_mag_preprocessed.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix of the Missing Conferences Locations\n",
    "Some papers have only the indication of the conference series. For this reason, the conference instance and the related conference locations don't have a value.\n",
    "\n",
    "However, every paper has been published in a specific \"instance\" of a conference, hence it should have a location. These papers will be \"fixed\" considering the year of their publication and their conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed_subset = df_mag_preprocessed.iloc[:50]\n",
    "df_mag_preprocessed_subset = df_mag_preprocessed_subset.dropna(subset = ['ConferenceNormalizedName'])\n",
    "df_mag_preprocessed_subset.iloc[:10][[\"Year\", \"ConferenceSeriesNormalizedName\", \"ConferenceNormalizedName\", \"ConferenceDisplayName\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above test, the ConferenceNormalizedName seems to be made by the concatenation of ConferenceSeriesNormalizedName in lowercase, a space, and the papers' year.\n",
    "\n",
    "**Note**: in the above subset the ConferenceDisplayName seems to be composed in the same way of ConferenceNormalizedName, but without the lowercase. However, this is not always true!\n",
    "\n",
    "Now we're going to populate the ConferenceNormalizedName instances that don't have a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed.ConferenceNormalizedName.fillna(df_mag_preprocessed.ConferenceSeriesNormalizedName.str.lower() + ' ' + df_mag_preprocessed.Year.astype(str), inplace=True)\n",
    "df_mag_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to do a new merge with the Conference Instances dataframe (this time it will be made on the ConferenceNormalizedName column), but I had no luck: these conference instances are missing. That's probably the reason of the NaN values in the ConferenceInstanceID field of the original Papers table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conf_instances.loc[df_mag_conf_instances[\"ConferenceNormalizedName\"] == \"enter 2013\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering the Missing Conferences Locations from the DBLP Website\n",
    "The missing conferences locations are going to be obtained from queries to the DBLP Website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conferences = df_mag_preprocessed[[\"ConferenceNormalizedName\", \"ConferenceLocation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Test: salvataggio del subset del dataframe per velocizzare le operazioni\n",
    "\n",
    "# Save on disk\n",
    "df_mag_conferences.to_csv(path_file_export + 'out_mag_tmp_conferences.csv')\n",
    "print(f'Successfully Exported the Preprocessed CSV to {path_file_export}out_mag_tmp_conferences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PaperID</th>\n",
       "      <th>ConferenceNormalizedName</th>\n",
       "      <th>ConferenceLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14558443</td>\n",
       "      <td>disc 2014</td>\n",
       "      <td>Austin, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15354235</td>\n",
       "      <td>esa 2014</td>\n",
       "      <td>Wrocław, Poland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24327294</td>\n",
       "      <td>enter 2013</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>60437532</td>\n",
       "      <td>dexa 2002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>198056957</td>\n",
       "      <td>icaisc 2006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409807</th>\n",
       "      <td>4409811</td>\n",
       "      <td>3102242761</td>\n",
       "      <td>iecon 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409808</th>\n",
       "      <td>4409812</td>\n",
       "      <td>3136855299</td>\n",
       "      <td>bmsb 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409809</th>\n",
       "      <td>4409813</td>\n",
       "      <td>3145351916</td>\n",
       "      <td>acc 1988</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409810</th>\n",
       "      <td>4409814</td>\n",
       "      <td>3151696876</td>\n",
       "      <td>icassp 2002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409811</th>\n",
       "      <td>4409815</td>\n",
       "      <td>3162646375</td>\n",
       "      <td>icit 2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4409812 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0     PaperID ConferenceNormalizedName ConferenceLocation\n",
       "0                 0    14558443                disc 2014         Austin, TX\n",
       "1                 1    15354235                 esa 2014    Wrocław, Poland\n",
       "2                 2    24327294               enter 2013                NaN\n",
       "3                 3    60437532                dexa 2002                NaN\n",
       "4                 4   198056957              icaisc 2006                NaN\n",
       "...             ...         ...                      ...                ...\n",
       "4409807     4409811  3102242761               iecon 2020                NaN\n",
       "4409808     4409812  3136855299                bmsb 2020                NaN\n",
       "4409809     4409813  3145351916                 acc 1988                NaN\n",
       "4409810     4409814  3151696876              icassp 2002                NaN\n",
       "4409811     4409815  3162646375                icit 2020                NaN\n",
       "\n",
       "[4409812 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Test: lettura del subset del dataframe per velocizzare le operazioni ---> cCODICE DA RIMUOVERE\n",
    "#df_mag_conferences = pd.read_csv(path_file_export + 'out_mag_tmp_conferences.csv', low_memory=False)\n",
    "df_mag_conferences = pd.read_csv(path_file_export + 'out_mag_tmp_locations_subset.csv', low_memory=False)\n",
    "\n",
    "# TODO Test: drop della colonna paper id che non ci serve più ---> cCODICE DA RIMUOVERE\n",
    "df_mag_conferences = df_mag_conferences[[\"ConferenceNormalizedName\", \"ConferenceLocation\"]]\n",
    "df_mag_conferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of the duplicated conferences. We only need unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of the papers that don't need their location to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ConferenceNormalizedName</th>\n",
       "      <th>ConferenceLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enter 2013</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dexa 2002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>icaisc 2006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interact 2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fct 2005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397042</th>\n",
       "      <td>dis 2001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4404554</th>\n",
       "      <td>syscon 2017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405987</th>\n",
       "      <td>iwaal 2016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407733</th>\n",
       "      <td>pppj 2002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409381</th>\n",
       "      <td>escience 2015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29253 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ConferenceNormalizedName ConferenceLocation\n",
       "2                     enter 2013                NaN\n",
       "3                      dexa 2002                NaN\n",
       "4                    icaisc 2006                NaN\n",
       "5                  interact 2011                NaN\n",
       "6                       fct 2005                NaN\n",
       "...                          ...                ...\n",
       "4397042                 dis 2001                NaN\n",
       "4404554              syscon 2017                NaN\n",
       "4405987               iwaal 2016                NaN\n",
       "4407733                pppj 2002                NaN\n",
       "4409381            escience 2015                NaN\n",
       "\n",
       "[29253 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mag_conferences = df_mag_conferences[df_mag_conferences[\"ConferenceLocation\"].isna()]\n",
    "df_mag_conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we only need to search for the location of 40361 unique conferences\n"
     ]
    }
   ],
   "source": [
    "df_mag_conferences = df_mag_conferences.drop_duplicates(subset=\"ConferenceNormalizedName\")\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_preprocessed_tmp_locations_subset.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel execution of the queries to the DBLP website.\n",
    "\n",
    "**Note**: this operation should take less than 10min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_url = \"https://dblp.org/db/\"\n",
    "dict_conf_locations = {}      \n",
    "download_list = list(df_mag_conferences.ConferenceNormalizedName.values)\n",
    "\n",
    "executor = concurrent.futures.ProcessPoolExecutor(max_workers=n_cpu_threads * 10, mp_context=mp_ctx)\n",
    "futures = [executor.submit(mt_get_mag_conf_location_from_dblp_operation, conf_name, dblp_url) for conf_name in download_list]\n",
    "\n",
    "for future in concurrent.futures.as_completed(futures):\n",
    "    try:\n",
    "        k, v = future.result()\n",
    "    except Exception as e:\n",
    "        print(f\"{futures[future]} throws {e}\")\n",
    "    else:\n",
    "        dict_conf_locations[k] = v\n",
    "        pass\n",
    "\n",
    "# Converting the resulting dictionary to a dataframe\n",
    "df_conf_locations = pd.DataFrame(dict_conf_locations.items(), columns=['ConferenceNormalizedName', 'ConferenceLocation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many conference locations have been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loc = df_conf_locations.copy()\n",
    "tmp_loc = tmp_loc.dropna(subset = ['ConferenceLocation'])\n",
    "\n",
    "print(f\"Fixed {len(tmp_loc.index)} over {len(df_conf_locations.index)} unique conferences\")\n",
    "\n",
    "tmp_loc = None # Free some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join of the new location data with the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed = pd.merge(df_mag_papers, df_conf_locations, on=['ConferenceNormalizedName'], how='left')\n",
    "df_mag_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of how many paper's conference locations are still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing = len(df_mag_preprocessed.dropna(subset = ['ConferenceLocation']).index) - len(df_mag_preprocessed_no_location.index)\n",
    "print(f\"{n_missing} missing locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write of the Final CSV on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write of the resulting CSV on Disk\n",
    "df_mag_preprocessed.to_csv(path_file_export + 'out_mag_citations_count_and_conferences.csv')\n",
    "print(f'Successfully Exported the Preprocessed CSV to {path_file_export}out_mag_citations_count_and_conferences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check of the Exported CSV to be sure that everything went fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of the Exported CSV\n",
    "df_mag_exported_csv = pd.read_csv(path_file_export + 'out_mag_citations_count_and_conferences.csv', low_memory=False)\n",
    "df_mag_exported_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order by citations count descending to see the articles with the most citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by citations count descending to see the articles with the most citations\n",
    "df_mag_exported_csv = df_mag_exported_csv.sort_values(by='CitationCount', ascending=False)\n",
    "df_mag_exported_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
