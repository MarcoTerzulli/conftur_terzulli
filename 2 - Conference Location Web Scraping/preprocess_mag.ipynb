{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocess Microsoft Academics Graph (MAG) Dataset\n",
    "\n",
    "Jupyter Notebook for the preprocessing of the Microsoft Academics Graph (MAG) dump.\n",
    "\n",
    "For this process, the following CSV files are needed: ```ConferenceInstances.txt```, ```ConferenceSeries.txt```, ```Papers.txt```. \n",
    "The above files can be found here: https://archive.org/download/mag-2021-06-07/mag/\n",
    "\n",
    "In particular, the following operations are going to be executed:\n",
    "* Opening of ConferenceInstances and ConferenceSeries CSVs\n",
    "* Drop of the useless columns \n",
    "* Chuncked Processing of the Papers CSV\n",
    "    * Drop of the useless columns\n",
    "    * Drop of papers without DOI\n",
    "    * Drop of papers from journals and books rows\n",
    "* Merge with the processed conferences data\n",
    "* Fix of some mismatched data types\n",
    "* Fix of some missing conferences locations with queries to the DBLP website\n",
    "\n",
    "Lastly, the entire preprocessed dump is going to be saved on disk in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import pandas as pd\n",
    "import platform\n",
    "import multiprocessing as mp \n",
    "import concurrent       \n",
    "from preprocess_multithread_utils import * \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "Please set your working directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PATHS ********************+\n",
    "\n",
    "# Dumps Directory Path\n",
    "path_file_import = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Import/'\n",
    "\n",
    "# CSV Exports Directory Path\n",
    "path_file_export = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Previuously Preprocessed Papers CSV\n",
    "This can be really useful to save some time using a previously elaborated CSV file. We don't need to repeat the same operations!\n",
    "\n",
    "**Note**: the CSV needs to be in the same format of the one generated with this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Previuously Preprocessed Papers CSV\n",
    "#\n",
    "# This can be really useful to save some time using a previously elaborated\n",
    "# CSV file. We don't need to repeat the same operations!\n",
    "#\n",
    "# Note: the CSV needs to be in the same format of the one generated with this script\n",
    "read_preprocessed_papers = True\n",
    "preprocessed_papers_csv_path = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/out_mag_papers.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading Settings\n",
    "Settings needed for the multithreaded queries to gather the missing conferences locations from the DBLP website.\n",
    "\n",
    "Please specify the number of CPU threads below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu_threads = 8 # Number of CPU threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special setting for the specific operating systems.\n",
    "\n",
    "**Note**: Due to the latest MacOS releases' security measures, we need to use the spawn method instead of fork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Notebook running on {platform.system()} OS: \")\n",
    "\n",
    "if platform.system() == \"Darwin\" or platform.system() == \"Windows\": # MacOS and windows\n",
    "    mp_ctx = mp.get_context(\"spawn\")\n",
    "    print(\"Spawn method has been set\")\n",
    "    \n",
    "else: # other unix systems\n",
    "    mp_ctx = mp.get_context(\"fork\")\n",
    "    print(\"Spawn method has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess of Conference Instances CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE INSTANCES ********************\n",
    "\n",
    "# Read of the Conference Instances File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_instances_col_names = ['ConferenceInstanceID', 'NormalizedName', 'DisplayName', 'ConferenceSeriesID', 'Location', 'OfficialUrl', 'StartDate', 'EndDate', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_instances = pd.read_csv(path_file_import + 'ConferenceInstances.txt', sep='\\t', names=df_mag_conf_instances_col_names)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the useless columns are going to be removed from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Instances' Useless Columns\n",
    "df_mag_conf_instances = df_mag_conf_instances.drop(columns=['OfficialUrl', 'AbstractRegistrationDate', 'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate', 'PageCount', 'PaperFamilyCount', 'CitationCount', 'Latitude', 'Longitude', 'CreatedDate', 'StartDate', 'EndDate'])\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column rename to remove ambiguity for the future joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_instances.rename(columns={'NormalizedName': 'ConferenceNormalizedName', 'DisplayName': 'ConferenceDisplayName', 'Location': 'ConferenceLocation'}, inplace=True)\n",
    "df_mag_conf_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess of Conference Series CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* CONFERENCE SERIES ********************\n",
    "\n",
    "# Read of the Conference Series File\n",
    "\n",
    "# The column names follow the MAG' scheme official documentation\n",
    "df_mag_conf_series_col_names = ['ConferenceSeriesID', 'Rank', 'NormalizedName', 'DisplayName', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "df_mag_conf_series = pd.read_csv(path_file_import + 'ConferenceSeries.txt', sep='\\t', names=df_mag_conf_series_col_names)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the useless columns are going to be removed from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop of Conference Series' Useless Columns\n",
    "df_mag_conf_series = df_mag_conf_series.drop(columns=['Rank', 'PaperCount', 'PaperFamilyCount', 'CitationCount', 'CreatedDate'])\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column rename to remove ambiguity for the future joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column rename to remove ambiguity for the future joins\n",
    "df_mag_conf_series.rename(columns={'NormalizedName': 'ConferenceSeriesNormalizedName', 'DisplayName': 'ConferenceSeriesDisplayName'}, inplace=True)\n",
    "df_mag_conf_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess of Papers CSV\n",
    "The Papers CSV is going to be processed in chunks, due to its size.\n",
    "\n",
    "The following operations are going to be executed:\n",
    "* Drop of the useless columns\n",
    "* Filtering of papers without DOI\n",
    "* Filtering papers that are not related to conferences\n",
    "* Drop of the doctype column\n",
    "* Write of the processed file on disk (in CSV format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PAPERS ********************\n",
    "\n",
    "# Read of previously prerocessed CSV\n",
    "df_mag_papers = None\n",
    "if read_preprocessed_papers:\n",
    "    df_mag_papers = pd.read_csv(preprocessed_papers_csv_path, low_memory=False, index_col=0)\n",
    "else:\n",
    "    # The Papers CSV is going to be processed in chunks, due to its size\n",
    "\n",
    "    # The column names follow the MAG' scheme official documentation\n",
    "    df_mag_papers_col_names = ['PaperID', 'Rank', 'Doi', 'DocType', 'PaperTitle', 'OriginalTitle', 'BookTitle', 'Year', 'Date', 'OnlineDate', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'ConferenceInstanceID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'EstimatedCitation', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate']\n",
    "\n",
    "    # List of processed chunks.\n",
    "    df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "    # Define of the chunk size\n",
    "    chunksize = 10 ** 7\n",
    "\n",
    "    count = 1\n",
    "    with pd.read_csv(path_file_import + 'Papers.txt', sep='\\t', chunksize=chunksize, low_memory=False, on_bad_lines='skip', names=df_mag_papers_col_names) as reader:\n",
    "        for chunk in reader:\n",
    "\n",
    "            # Drop of the useless columns\n",
    "            chunk = chunk.drop(columns=['Rank', 'OnlineDate', 'Publisher', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'OriginalVenue', 'FamilyID', 'FamilyRank', 'Retracion', 'CreatedDate', 'JournalID', 'BookTitle', 'Date'])\n",
    "\n",
    "            # Filtering of papers without DOI\n",
    "            chunk = chunk.dropna(subset = ['Doi'])\n",
    "\n",
    "            # Filtering papers that are not related to conferences\n",
    "            chunk = chunk[chunk.DocType == 'Conference']\n",
    "\n",
    "            # Drop of the doctype column\n",
    "            chunk = chunk.drop(columns=['DocType'])\n",
    "\n",
    "            # Insert of the resulting chunk in the list \n",
    "            df_mag_papers_list_of_chunks.append(chunk)\n",
    "\n",
    "            print(f'Successfully processed chunk {count} out of around {260000000 / chunksize}')\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "    # Concatenation of the processed chunks\n",
    "    df_mag_papers = pd.concat(df_mag_papers_list_of_chunks)\n",
    "\n",
    "    # Empty the list to free some memory\n",
    "    df_mag_papers_list_of_chunks = list()\n",
    "\n",
    "    # Write of the resulting CSV on Disk\n",
    "    df_mag_papers.to_csv(path_file_export + 'out_mag_papers.csv')\n",
    "    print(f'Successfully Exported the Preprocessed Papers CSV to {path_file_export}out_mag_papers.csv')\n",
    "\n",
    "df_mag_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge of Conferences and Papers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* MERGE OF CONFERENCES AND PAPERS DATA ********************\n",
    "\n",
    "# Merge of conferences and papers data over the conferenceseries id columnn to get the conference series name\n",
    "# The papers' row that will not match will be preserved\n",
    "df_mag_preprocessed = pd.merge(df_mag_papers, df_mag_conf_series, on=['ConferenceSeriesID'], how='left')\n",
    "\n",
    "# Merge of conferences and papers data over the conferenceinstances id columnn\n",
    "# The papers' row that will not match will be preserved\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_mag_conf_instances, on=['ConferenceInstanceID'], how='left')\n",
    "\n",
    "# Drop of the duplicated columns\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceSeriesID_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceSeriesID_x':'ConferenceSeriesID'}, inplace=True)\n",
    "\n",
    "# Removing broken data (four records seems to have mismatched types in some columns)\n",
    "df_mag_preprocessed = df_mag_preprocessed.dropna(subset = ['CitationCount'])\n",
    "\n",
    "df_mag_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix of the Mismatched Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix of the year data type, that has been interpreted as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_mag_preprocessed.iloc[:1][\"Year\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed = df_mag_preprocessed.astype({\"Year\": int}, errors='raise') \n",
    "df_mag_preprocessed.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix of the CitationCount data type, that has been interpreted as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_mag_preprocessed.iloc[:1][\"CitationCount\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed = df_mag_preprocessed.astype({\"CitationCount\": int}, errors='raise') \n",
    "df_mag_preprocessed.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix of the Missing Conferences Locations\n",
    "Some papers have only the indication of the conference series. For this reason, the conference instance and the related conference locations don't have a value.\n",
    "\n",
    "However, every paper has been published in a specific \"instance\" of a conference, hence it should have a location. These papers will be \"fixed\" considering the year of their publication and their conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed_subset = df_mag_preprocessed.iloc[:50]\n",
    "df_mag_preprocessed_subset = df_mag_preprocessed_subset.dropna(subset = ['ConferenceNormalizedName'])\n",
    "df_mag_preprocessed_subset.iloc[:10][[\"Year\", \"ConferenceSeriesNormalizedName\", \"ConferenceNormalizedName\", \"ConferenceDisplayName\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above test, the ConferenceNormalizedName seems to be made by the concatenation of ConferenceSeriesNormalizedName in lowercase, a space, and the papers' year.\n",
    "\n",
    "**Note**: in the above subset the ConferenceDisplayName seems to be composed in the same way of ConferenceNormalizedName, but without the lowercase. However, this is not always true!\n",
    "\n",
    "Now we're going to populate the ConferenceNormalizedName instances that don't have a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed.ConferenceNormalizedName.fillna(df_mag_preprocessed.ConferenceSeriesNormalizedName.str.lower() + ' ' + df_mag_preprocessed.Year.astype(str), inplace=True)\n",
    "df_mag_preprocessed.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to do a new merge with the Conference Instances dataframe (this time it will be made on the ConferenceNormalizedName column), but I had no luck: these conference instances are missing. That's probably the reason of the NaN values in the ConferenceInstanceID field of the original Papers table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conf_instances.loc[df_mag_conf_instances[\"ConferenceNormalizedName\"] == \"enter 2013\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Missing Conferences Locations from the DBLP Website\n",
    "The missing conferences locations are going to be obtained from queries to the DBLP Website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conferences = df_mag_preprocessed[[\"ConferenceNormalizedName\", \"ConferenceLocation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO TEST\n",
    "df_mag_conferences = pd.read_csv(path_file_export + 'out_mag_tmp_locations_subset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of the papers that don't need their location to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conferences = df_mag_conferences[df_mag_conferences[\"ConferenceLocation\"].isna()]\n",
    "df_mag_conferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of the duplicated conferences. We only need unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conferences = df_mag_conferences.drop_duplicates(subset=\"ConferenceNormalizedName\")\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define of the Web Scraping Function\n",
    "We'll do a web scraping in two different URL formats, hence the need of two web scraping functions (that are going to be passed as parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dblp_location_scraper(conferences_dataframe, mt_downloader_operation_function, dblp_url = \"https://dblp.org/db/conf/\"):\n",
    "    dict_conf_locations = {}      \n",
    "    download_list = list(conferences_dataframe.ConferenceNormalizedName.values)\n",
    "\n",
    "    executor = concurrent.futures.ProcessPoolExecutor(max_workers=n_cpu_threads * 10, mp_context=mp_ctx)\n",
    "    futures = [executor.submit(mt_downloader_operation_function, conf_name, dblp_url) for conf_name in download_list]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            k, v = future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"{futures[future]} throws {e}\")\n",
    "        else:\n",
    "            dict_conf_locations[k] = v\n",
    "            pass\n",
    "\n",
    "    # Converting the resulting dictionary to a dataframe\n",
    "    df_conf_locations = pd.DataFrame(dict_conf_locations.items(), columns=['ConferenceNormalizedName', 'ConferenceLocation'])\n",
    "\n",
    "    return df_conf_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/conf/CONF_NAME/CONF_NAMEYEAR.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel execution of the queries to the DBLP website.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TEST\n",
    "df_mag_conferences_v1_1_subset = df_mag_conferences.iloc[0:1000]\n",
    "df_conf_locations_v1_1 = dblp_location_scraper(df_mag_conferences_v1_1_subset, mt_get_mag_conf_location_from_dblp_operation_v1, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf_locations_v1_1 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v1, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many conference locations have been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf_locations_v1_1 = df_conf_locations_v1_1.dropna(subset = ['ConferenceLocation'])\n",
    "\n",
    "print(f\"Fixed {len(df_conf_locations_v1_1.index)} over {len(df_mag_conferences.index)} unique conferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/series/CONF_NAME/CONF_NAMEYEAR.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try to get more location composing the URL in a different way.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in my tests, this method gave no results. I decided to leave the original code, in case something will change on the DBLP website. You can execute the download anyway if you want, by editing the following value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_anyway = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to filter the conferences that have already been obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = df_mag_conferences[\"ConferenceNormalizedName\"].isin(df_conf_locations_v1_1[\"ConferenceNormalizedName\"])\n",
    "df_mag_conferences.drop(df_mag_conferences[rows_to_drop].index, inplace=True)\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_anyway:\n",
    "    df_conf_locations_v1_2 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v1, \"https://dblp.org/db/series/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many conference locations have been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_anyway:\n",
    "    df_conf_locations_v1_2 = df_conf_locations_v1_2.dropna(subset = ['ConferenceLocation'])\n",
    "\n",
    "    print(f\"Fixed {len(df_conf_locations_v1_2.index)} over {len(df_mag_conferences.index)} unique conferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/conf/CONF_NAME/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel execution of the queries to the DBLP website.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to filter the conferences that have already been obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_anyway:\n",
    "    rows_to_drop = df_mag_conferences[\"ConferenceNormalizedName\"].isin(df_conf_locations_v1_2[\"ConferenceNormalizedName\"])\n",
    "    df_mag_conferences.drop(df_mag_conferences[rows_to_drop].index, inplace=True)\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TEST\n",
    "df_mag_conferences_v2_1_subset = df_mag_conferences.iloc[0:10]\n",
    "df_mag_locations_v2_1 = dblp_location_scraper(df_mag_conferences_v2_1_subset, mt_get_mag_conf_location_from_dblp_operation_v2, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_locations_v2_1 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v2, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/series/CONF_NAME/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel execution of the queries to the DBLP website.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to filter the conferences that have already been obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = df_mag_conferences[\"ConferenceNormalizedName\"].isin(df_conf_locations_v2_1[\"ConferenceNormalizedName\"])\n",
    "df_mag_conferences.drop(df_mag_conferences[rows_to_drop].index, inplace=True)\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_locations_v2_2 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v2, \"https://dblp.org/db/series/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join of the New Location Data with the Original Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the first location dataframe\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v1_1, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "# Combine the two columns\n",
    "df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "\n",
    "if download_anyway:\n",
    "    # Merge with the second location dataframe\n",
    "    df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v1_2, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "    # Combine the two columns\n",
    "    df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "    df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "    df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "\n",
    "# Merge with the third location dataframe\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v2_1, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "# Combine the two columns\n",
    "df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "\n",
    "# Merge with the fourth location dataframe\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v2_2, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "# Combine the two columns\n",
    "df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "df_mag_preprocessed.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of how many paper's conference locations are still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing = len(df_mag_preprocessed.index) - len(df_mag_preprocessed.dropna(subset = ['ConferenceLocation']).index)\n",
    "print(f\"{n_missing} missing paper's conference locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write of the Final CSV on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write of the resulting CSV on Disk\n",
    "df_mag_preprocessed.to_csv(path_file_export + 'out_mag_citations_count_and_conferences.csv')\n",
    "print(f'Successfully Exported the Preprocessed CSV to {path_file_export}out_mag_citations_count_and_conferences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check of the Exported CSV to be sure that everything went fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of the Exported CSV\n",
    "df_mag_exported_csv = pd.read_csv(path_file_export + 'out_mag_citations_count_and_conferences.csv', low_memory=False)\n",
    "df_mag_exported_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order by citations count descending to see the articles with the most citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by citations count descending to see the articles with the most citations\n",
    "df_mag_exported_csv = df_mag_exported_csv.sort_values(by='CitationCount', ascending=False)\n",
    "df_mag_exported_csv.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
