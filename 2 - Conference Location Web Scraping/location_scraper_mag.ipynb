{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Location Web Scraping of Microsoft Academics Graph (MAG) Dataset\n",
    "\n",
    "Jupyter Notebook for the web scraping of the conferences locations of the Microsoft Academics Graph (MAG) dump.\n",
    "\n",
    "For this process, the following CSV file is needed: ```out_mag_citations_count_and_conferences.csv```. \n",
    "The above file must be generated running the ```preprocess_mag.ipynb``` Notebook that is contained in the ```1 - Citation Dumps Preprocess``` folder of this Repository.\n",
    "\n",
    "In particular, the following operations are going to be executed:\n",
    "* Opening of the CSV peprocessed dump\n",
    "* Fix of the conferences names\n",
    "* Obtaining the missing locations with queries to the DBLP website\n",
    "* Fix of the locations format\n",
    "\n",
    "Lastly, the entire preprocessed dump is going to be saved on disk in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import pandas as pd\n",
    "import platform\n",
    "import multiprocessing as mp \n",
    "import concurrent       \n",
    "from preprocess_multithread_utils import * \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "Please set your working directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* PATHS ********************+\n",
    "\n",
    "# Dumps Directory Path\n",
    "path_file_import = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Import/'\n",
    "\n",
    "# CSV Exports Directory Path\n",
    "path_file_export = r'/Users/marcoterzulli/File/Scuola Local/Magistrale/Materiale Corsi Attuali/Tirocinio/Cartella di Lavoro/Archivi Dump di Lavoro/Export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading Settings\n",
    "Settings needed for the multithreaded queries to gather the missing conferences locations from the DBLP website.\n",
    "\n",
    "Please specify the number of CPU threads below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu_threads = 8 # Number of CPU threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special setting for the specific operating systems.\n",
    "\n",
    "**Note**: Due to the latest MacOS releases' security measures, we need to use the spawn method instead of fork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Notebook running on {platform.system()} OS: \")\n",
    "\n",
    "if platform.system() == \"Darwin\" or platform.system() == \"Windows\": # MacOS and windows\n",
    "    mp_ctx = mp.get_context(\"spawn\")\n",
    "    print(\"Spawn method has been set\")\n",
    "    \n",
    "else: # other unix systems\n",
    "    mp_ctx = mp.get_context(\"fork\")\n",
    "    print(\"Spawn method has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read of the CSV Preprocessed Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed = pd.read_csv(path_file_export + 'out_mag_citations_count_and_conferences.csv', low_memory=False)\n",
    "df_mag_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix of the Missing Conferences Names\n",
    "Some papers have only the indication of the conference series. For this reason, the conference instance and the related conference locations don't have a value.\n",
    "\n",
    "However, every paper has been published in a specific \"instance\" of a conference, hence it should have a location. These papers will be \"fixed\" considering the year of their publication and their conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed_subset = df_mag_preprocessed.iloc[:50]\n",
    "df_mag_preprocessed_subset = df_mag_preprocessed_subset.dropna(subset = ['ConferenceNormalizedName'])\n",
    "df_mag_preprocessed_subset.iloc[:10][[\"Year\", \"ConferenceSeriesNormalizedName\", \"ConferenceNormalizedName\", \"ConferenceDisplayName\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above test, the ConferenceNormalizedName seems to be made by the concatenation of ConferenceSeriesNormalizedName in lowercase, a space, and the papers' year.\n",
    "\n",
    "**Note**: in the above subset the ConferenceDisplayName seems to be composed in the same way of ConferenceNormalizedName, but without the lowercase. However, this is not always true!\n",
    "\n",
    "Now we're going to populate the ConferenceNormalizedName instances that don't have a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_preprocessed.ConferenceNormalizedName.fillna(df_mag_preprocessed.ConferenceSeriesNormalizedName.str.lower() + ' ' + df_mag_preprocessed.Year.astype(str), inplace=True)\n",
    "df_mag_preprocessed.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to do a new merge with the Conference Instances dataframe (this time it will be made on the ConferenceNormalizedName column), but I had no luck: these conference instances are missing. That's probably the reason of the NaN values in the ConferenceInstanceID field of the original Papers table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conf_instances.loc[df_mag_conf_instances[\"ConferenceNormalizedName\"] == \"enter 2013\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Missing Conferences Locations from the DBLP Website\n",
    "The missing conferences locations are going to be obtained from queries to the DBLP Website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conferences = df_mag_preprocessed[[\"ConferenceNormalizedName\", \"ConferenceLocation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO TEST\n",
    "df_mag_conferences = pd.read_csv(path_file_export + 'out_mag_tmp_locations_subset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of the papers that don't need their location to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conferences = df_mag_conferences[df_mag_conferences[\"ConferenceLocation\"].isna()]\n",
    "df_mag_conferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of the duplicated conferences. We only need unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_conferences = df_mag_conferences.drop_duplicates(subset=\"ConferenceNormalizedName\")\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define of the Web Scraping Function\n",
    "We'll do a web scraping in two different URL formats, hence the need of two web scraping phases (with two different functions that are going to be passed as parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dblp_location_scraper(conferences_dataframe, mt_downloader_operation_function, dblp_url = \"https://dblp.org/db/conf/\"):\n",
    "    dict_conf_locations = {}      \n",
    "    download_list = list(conferences_dataframe.ConferenceNormalizedName.values)\n",
    "\n",
    "    executor = concurrent.futures.ProcessPoolExecutor(max_workers=n_cpu_threads * 10, mp_context=mp_ctx)\n",
    "    futures = [executor.submit(mt_downloader_operation_function, conf_name, dblp_url) for conf_name in download_list]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            k, v = future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"{futures[future]} throws {e}\")\n",
    "        else:\n",
    "            dict_conf_locations[k] = v\n",
    "            pass\n",
    "\n",
    "    # Converting the resulting dictionary to a dataframe\n",
    "    df_conf_locations = pd.DataFrame(dict_conf_locations.items(), columns=['ConferenceNormalizedName', 'ConferenceLocation'])\n",
    "\n",
    "    return df_conf_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/conf/CONF_NAME/CONF_NAMEYEAR.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel execution of the queries to the DBLP website.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TEST\n",
    "df_mag_conferences_v1_1_subset = df_mag_conferences.iloc[0:1000]\n",
    "df_conf_locations_v1_1 = dblp_location_scraper(df_mag_conferences_v1_1_subset, mt_get_mag_conf_location_from_dblp_operation_v1, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf_locations_v1_1 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v1, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many conference locations have been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf_locations_v1_1 = df_conf_locations_v1_1.dropna(subset = ['ConferenceLocation'])\n",
    "\n",
    "print(f\"Fixed {len(df_conf_locations_v1_1.index)} over {len(df_mag_conferences.index)} unique conferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/series/CONF_NAME/CONF_NAMEYEAR.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try to get more location composing the URL in a different way.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in my tests, this method gave no results. I decided to leave the original code, in case something will change on the DBLP website. You can execute the download anyway if you want, by editing the following value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_anyway = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to filter the conferences that have already been obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = df_mag_conferences[\"ConferenceNormalizedName\"].isin(df_conf_locations_v1_1[\"ConferenceNormalizedName\"])\n",
    "df_mag_conferences.drop(df_mag_conferences[rows_to_drop].index, inplace=True)\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_anyway:\n",
    "    df_conf_locations_v1_2 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v1, \"https://dblp.org/db/series/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many conference locations have been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_anyway:\n",
    "    df_conf_locations_v1_2 = df_conf_locations_v1_2.dropna(subset = ['ConferenceLocation'])\n",
    "\n",
    "    print(f\"Fixed {len(df_conf_locations_v1_2.index)} over {len(df_mag_conferences.index)} unique conferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/conf/CONF_NAME/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel execution of the queries to the DBLP website.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to filter the conferences that have already been obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_anyway:\n",
    "    rows_to_drop = df_mag_conferences[\"ConferenceNormalizedName\"].isin(df_conf_locations_v1_2[\"ConferenceNormalizedName\"])\n",
    "    df_mag_conferences.drop(df_mag_conferences[rows_to_drop].index, inplace=True)\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TEST\n",
    "df_mag_conferences_v2_1_subset = df_mag_conferences.iloc[0:10]\n",
    "df_mag_locations_v2_1 = dblp_location_scraper(df_mag_conferences_v2_1_subset, mt_get_mag_conf_location_from_dblp_operation_v2, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_locations_v2_1 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v2, \"https://dblp.org/db/conf/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries to https://dblp.org/db/series/CONF_NAME/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel execution of the queries to the DBLP website.\n",
    "\n",
    "**Note**: this operation should take less than 20min, depending on your Internet speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to filter the conferences that have already been obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = df_mag_conferences[\"ConferenceNormalizedName\"].isin(df_conf_locations_v2_1[\"ConferenceNormalizedName\"])\n",
    "df_mag_conferences.drop(df_mag_conferences[rows_to_drop].index, inplace=True)\n",
    "\n",
    "print(f\"Now we only need to search for the location of {df_mag_conferences.__len__()} unique conferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_locations_v2_2 = dblp_location_scraper(df_mag_conferences, mt_get_mag_conf_location_from_dblp_operation_v2, \"https://dblp.org/db/series/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join of the New Location Data with the Original Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the first location dataframe\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v1_1, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "# Combine the two columns\n",
    "df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "\n",
    "if download_anyway:\n",
    "    # Merge with the second location dataframe\n",
    "    df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v1_2, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "    # Combine the two columns\n",
    "    df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "    df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "    df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "\n",
    "# Merge with the third location dataframe\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v2_1, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "# Combine the two columns\n",
    "df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "\n",
    "# Merge with the fourth location dataframe\n",
    "df_mag_preprocessed = pd.merge(df_mag_preprocessed, df_conf_locations_v2_2, on=['ConferenceNormalizedName'], how='left')\n",
    "\n",
    "# Combine the two columns\n",
    "df_mag_preprocessed['ConferenceLocation_x'] = df_mag_preprocessed['ConferenceLocation_x'].fillna(df_mag_preprocessed['ConferenceLocation_y'])\n",
    "df_mag_preprocessed.rename(columns = {'ConferenceLocation_x':'ConferenceLocation'}, inplace=True)\n",
    "df_mag_preprocessed = df_mag_preprocessed.drop(columns=['ConferenceLocation_y'])\n",
    "\n",
    "df_mag_preprocessed.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of how many paper's conference locations are still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing = len(df_mag_preprocessed.index) - len(df_mag_preprocessed.dropna(subset = ['ConferenceLocation']).index)\n",
    "print(f\"{n_missing} missing paper's conference locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write of the Final CSV on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write of the resulting CSV on Disk\n",
    "df_mag_preprocessed.to_csv(path_file_export + 'out_mag_citations_and_locations.csv')\n",
    "print(f'Successfully Exported the Preprocessed CSV to {path_file_export}out_mag_citations_and_locations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check of the Exported CSV to be sure that everything went fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of the Exported CSV\n",
    "df_mag_exported_csv = pd.read_csv(path_file_export + 'out_mag_citations_and_locations.csv', low_memory=False)\n",
    "df_mag_exported_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order by citations count descending to see the articles with the most citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by citations count descending to see the articles with the most citations\n",
    "df_mag_exported_csv = df_mag_exported_csv.sort_values(by='CitationCount', ascending=False)\n",
    "df_mag_exported_csv.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
